schema: "ai-tasks/v0.3"
version: "2.1"
project: "rag-eval-foundations"
description: "Implement versioned golden testsets with PostgreSQL + Phoenix, quality gates, and CI/CD."

providers:
  planning_assistants: ["claude-code", "cursor", "continue.dev"]
  execution_targets:
    - type: "github-actions"
      workflow: ".github/workflows/golden-testset-ci.yaml"
    - type: "prefect"
      flow: "flows/golden_testset_flow.py"

global:
  env:
    PHOENIX_ENDPOINT: "${PHOENIX_ENDPOINT:-http://localhost:6006}"
    DATABASE_URL: "${DATABASE_URL:-postgresql://langchain:langchain@localhost:6024/langchain}"
    POSTGRES_HOST: "${POSTGRES_HOST:-localhost}"
    POSTGRES_PORT: "${POSTGRES_PORT:-6024}"
    POSTGRES_USER: "${POSTGRES_USER:-langchain}"
    POSTGRES_PASSWORD: "${POSTGRES_PASSWORD:-langchain}"
    POSTGRES_DB: "${POSTGRES_DB:-langchain}"
    RAGAS_MODEL: "gpt-4.1-mini"
    EMBEDDING_MODEL: "text-embedding-3-small"
    GOLDEN_TESTSET_SIZE: "${GOLDEN_TESTSET_SIZE:-10}"
  secrets:
    - OPENAI_API_KEY
    - COHERE_API_KEY
    - PHOENIX_API_KEY
    - PHOENIX_CLIENT_HEADERS
  quality_gates:
    critical:
      - "db_txn_atomic"
      - "no_data_loss_migration"
      - "enforce_quality_validation"
      - "version_conflicts_prevented"
    performance:
      - "p95_query_lt_2s"
      - "phoenix_upload_lt_30s"
      - "generation_lt_5min"
    quality:
      - "coverage_gte_90"
      - "diversity_score_gte_0.7"
      - "no_duplicate_questions"
    operational:
      - "cli_operational"
      - "rollback_tested"
      - "monitoring_active"
  success_criteria:
    - metric: "reliability"
      target: "99.9% retrieval uptime"
      assert: "python scripts/check_uptime.py --target 99.9"
    - metric: "performance"
      target: "<5s all operations"
      assert: "pytest tests/performance -q --benchmark-max-time=5"
    - metric: "quality"
      target: "All testsets pass validation"
      assert: "python scripts/verify_quality_gates.py --all"
    - metric: "cost"
      target: "Within 20% of baseline"
      assert: "python scripts/check_cost_variance.py --threshold 0.2"
    - metric: "adoption"
      target: "Migration completed"
      assert: "python scripts/verify_migration.py --expect active_version"

phases:
  - id: phase1
    name: "Database Schema & Infrastructure"
    depends_on: []
    tasks:
      - id: p1.sql.migrate
        name: "Create SQL schema + indexes"
        run:
          - "mkdir -p schemas"
          - "python scripts/generate_schema.py --output schemas/golden_testset_schema.sql"
          - "python scripts/generate_indexes.py --output schemas/golden_testset_indexes.sql"
          - "psql \"$DATABASE_URL\" -f schemas/golden_testset_schema.sql"
          - "psql \"$DATABASE_URL\" -f schemas/golden_testset_indexes.sql"
        verify:
          - "python scripts/db/assert_schema.py --require tables=golden_testset_versions,golden_testset_metadata,golden_testset_quality_metrics,golden_testset_audit_log"
          - "python scripts/db/assert_schema.py --require views=current_golden_testset"
          - "python scripts/db/assert_indexes.py --check performance"
      - id: p1.sql.rollback
        name: "Prepare rollback script"
        artifacts:
          - "schemas/rollback_golden_testset.sql"
        run:
          - "python scripts/generate_rollback.py --output schemas/rollback_golden_testset.sql"
        verify:
          - "python scripts/db/dry_run_sql.py schemas/rollback_golden_testset.sql --check syntax"
          - "python scripts/db/test_rollback.py --sandbox"
      - id: p1.db.conn
        name: "Async connection management"
        run:
          - "mkdir -p src/golden_testset"
          - "python scripts/generate_db_connection.py --output src/golden_testset/db_connection.py"
          - "ruff check src/golden_testset/db_connection.py --fix"
          - "ruff format src/golden_testset/db_connection.py"
        verify:
          - "pytest tests/test_db_connection.py -q --benchmark-disable"
          - "python scripts/test_connection_pool.py --min 5 --max 20"

  - id: phase2
    name: "Core Manager & Versioning"
    depends_on: [phase1]
    tasks:
      - id: p2.manager
        name: "GoldenTestsetManager implementation"
        run:
          - "python scripts/generate_manager.py --output src/golden_testset/manager.py"
          - "python scripts/generate_manager_tests.py --output tests/test_golden_testset_manager.py"
          - "ruff check src/golden_testset/manager.py --fix"
          - "pytest tests/test_golden_testset_manager.py -q --maxfail=1"
        verify:
          - "python scripts/db/assert_invariants.py --unique version"
          - "python scripts/db/assert_invariants.py --atomic transactions"
          - "pytest tests/test_golden_testset_manager.py --cov=src.golden_testset.manager --cov-fail-under=90"
      - id: p2.versioning
        name: "Semantic versioning logic"
        run:
          - "python scripts/generate_versioning.py --output src/golden_testset/versioning.py"
          - "python scripts/generate_versioning_tests.py --output tests/test_versioning.py"
          - "ruff check src/golden_testset/versioning.py --fix"
          - "pytest tests/test_versioning.py -q"
        verify:
          - "python scripts/test_version_bumping.py --rule major,minor,patch"
          - "python scripts/test_version_conflicts.py --expect prevented"
      - id: p2.changedetect
        name: "Document change detection"
        run:
          - "python scripts/generate_change_detector.py --output src/golden_testset/change_detector.py"
          - "python scripts/generate_change_tests.py --output tests/test_change_detector.py"
          - "ruff check src/golden_testset/change_detector.py --fix"
          - "pytest tests/test_change_detector.py -q"
        perf_budget_ms: 100
        verify:
          - "pytest tests/test_change_detector.py --benchmark-max-time=0.1"
          - "python scripts/test_hash_consistency.py --iterations 100"

  - id: phase3
    name: "Quality Validation Pipeline"
    depends_on: [phase2]
    tasks:
      - id: p3.validator
        name: "Statistical + semantic validation"
        run:
          - "python scripts/generate_validator.py --output src/golden_testset/quality_validator.py"
          - "python scripts/generate_validator_tests.py --output tests/test_quality_validator.py"
          - "ruff check src/golden_testset/quality_validator.py --fix"
          - "pytest tests/test_quality_validator.py -q"
        accepts:
          - "diversity_score >= 0.7"
          - "no_duplicates"
          - "coverage_complete"
          - "chi_square_p_value > 0.05"
        verify:
          - "python scripts/test_validation_metrics.py --all"
          - "pytest tests/test_quality_validator.py --cov=src.golden_testset.quality_validator --cov-fail-under=90"
      - id: p3.pipeline
        name: "Pre/Post/Activation gates"
        run:
          - "python scripts/generate_pipeline.py --output src/golden_testset/validation_pipeline.py"
          - "python scripts/generate_pipeline_tests.py --output tests/test_validation_pipeline.py"
          - "ruff check src/golden_testset/validation_pipeline.py --fix"
          - "pytest tests/test_validation_pipeline.py -q"
        verify:
          - "python scripts/test_pipeline_gates.py --stage pre,post,activation"
          - "python scripts/test_quality_enforcement.py --strict"

  - id: phase4
    name: "Phoenix & Cost Integration"
    depends_on: [phase3]
    tasks:
      - id: p4.phoenix
        name: "Versioned Phoenix dataset upload"
        run:
          - "python scripts/generate_phoenix_integration.py --output src/golden_testset/phoenix_integration.py"
          - "python scripts/generate_phoenix_tests.py --output tests/integration/test_phoenix.py"
          - "ruff check src/golden_testset/phoenix_integration.py --fix"
          - "python -m src.golden_testset.phoenix_integration --dry-run"
          - "pytest tests/integration/test_phoenix.py -q --timeout=30"
        verify:
          - "python scripts/test_phoenix_upload.py --timeout 30"
          - "python scripts/test_dataset_versioning.py --phoenix"
      - id: p4.cost
        name: "Token usage + cost tracking"
        run:
          - "python scripts/generate_cost_tracker.py --output src/golden_testset/cost_tracker.py"
          - "python scripts/generate_cost_tests.py --output tests/test_cost_tracker.py"
          - "ruff check src/golden_testset/cost_tracker.py --fix"
          - "pytest tests/test_cost_tracker.py -q"
        verify:
          - "python scripts/test_token_accuracy.py --tolerance 0.01"
          - "python scripts/test_cost_calculation.py --model gpt-4.1-mini"
          - "python scripts/test_budget_alerts.py --threshold 0.8"
      - id: p4.tracing
        name: "OpenTelemetry instrumentation"
        run:
          - "python scripts/generate_tracing.py --output src/golden_testset/tracing.py"
          - "python scripts/generate_tracing_tests.py --output tests/test_tracing.py"
          - "ruff check src/golden_testset/tracing.py --fix"
          - "pytest tests/test_tracing.py -q"
        verify:
          - "python scripts/test_spans.py --coverage all_operations"
          - "python scripts/test_metrics_export.py --phoenix"

  - id: phase5
    name: "CLI Tools & Automation"
    depends_on: [phase4]
    tasks:
      - id: p5.cli
        name: "testset_cli commands"
        run:
          - "python scripts/generate_cli.py --output src/golden_testset/cli.py"
          - "python scripts/generate_cli_tests.py --output tests/test_cli.py"
          - "ruff check src/golden_testset/cli.py --fix"
          - "python -m src.golden_testset.cli create --size 2 --auto-promote --dry-run"
          - "python -m src.golden_testset.cli list-versions --format json"
          - "python -m src.golden_testset.cli activate --version 0.1.0 --dry-run"
          - "python -m src.golden_testset.cli rollback --to-version 0.1.0 --reason 'test' --dry-run"
          - "python -m src.golden_testset.cli compare --v1 0.1.0 --v2 0.2.0 --dry-run"
        verify:
          - "pytest tests/test_cli.py -q"
          - "python scripts/test_cli_help.py --all-commands"
      - id: p5.schedules
        name: "Weekly regeneration + A/B testing"
        run:
          - "mkdir -p workflows"
          - "python scripts/generate_workflows.py --output workflows/"
          - "python scripts/generate_workflow_tests.py --output tests/integration/test_workflows.py"
          - "pytest tests/integration/test_workflows.py -q"
        verify:
          - "python scripts/test_scheduled_jobs.py --cron"
          - "python scripts/test_ab_testing.py --iterations 10"
      - id: p5.wrapper
        name: "Shell wrapper script"
        run:
          - "mkdir -p scripts"
          - "python scripts/generate_wrapper.py --output scripts/testset_manager.sh"
          - "chmod +x scripts/testset_manager.sh"
          - "shellcheck scripts/testset_manager.sh || true"
        verify:
          - "bash scripts/testset_manager.sh --help"

  - id: phase6
    name: "Migration & Rollback"
    depends_on: [phase5]
    tasks:
      - id: p6.backup
        name: "Backup JSON + DB"
        run:
          - "mkdir -p backups"
          - "test -f golden_testset.json && cp golden_testset.json backups/golden_testset_$(date +%Y%m%d_%H%M%S).json || echo 'No JSON to backup'"
          - "test -f golden_testset.csv && cp golden_testset.csv backups/golden_testset_$(date +%Y%m%d_%H%M%S).csv || echo 'No CSV to backup'"
          - "pg_dump \"$DATABASE_URL\" > backups/langchain_$(date +%Y%m%d_%H%M%S).sql"
        verify:
          - "ls -la backups/"
          - "python scripts/verify_backups.py --dir backups"
      - id: p6.migrate
        name: "Migrate legacy JSON → DB"
        run:
          - "python scripts/generate_migration.py --output scripts/migrate_golden_testset.py"
          - "python scripts/migrate_golden_testset.py --input golden_testset.json --to db --version 0.1.0"
          - "python scripts/migrate_golden_testset.py --input golden_testset.csv --to db --version 0.1.1 || true"
        verify:
          - "python scripts/verify_migration.py --expect active_version"
          - "python scripts/verify_migration.py --expect data_integrity"
          - "python scripts/test_backward_compat.py"
      - id: p6.rollback_smoke
        name: "Rollback smoke test"
        run:
          - "python scripts/generate_rollback_test.py --output scripts/test_rollback.py"
          - "python scripts/test_rollback.py --create-test-version"
          - "python scripts/rollback_to_prev_version.py --yes"
          - "python scripts/verify_rollback.py --expect previous_active"
          - "python scripts/rollback_to_prev_version.py --undo"
        verify:
          - "python scripts/verify_rollback.py --expect latest_active"
          - "python scripts/test_rollback_idempotent.py"

  - id: phase7
    name: "Testing, Performance & CI/CD"
    depends_on: [phase6]
    tasks:
      - id: p7.unit
        name: "Unit tests ≥ 90% coverage"
        run:
          - "pytest tests/unit -q --cov=src.golden_testset --cov-report=term-missing --cov-fail-under=90"
          - "pytest tests/unit -q --cov=src.golden_testset --cov-report=html"
        verify:
          - "python scripts/check_coverage.py --min 90"
          - "python scripts/check_test_quality.py"
      - id: p7.integration
        name: "Integration tests"
        run:
          - "pytest tests/integration -q --timeout=60"
        verify:
          - "python scripts/verify_integration.py --all"
      - id: p7.perf
        name: "Performance harness (P95 < 2s)"
        run:
          - "pytest tests/performance -q --benchmark-only --benchmark-max-time=2"
          - "python scripts/generate_perf_report.py --output reports/performance.md"
        verify:
          - "python scripts/check_p95_latency.py --max 2.0"
          - "python scripts/check_throughput.py --min 100"
      - id: p7.ci
        name: "GitHub Actions CI"
        run:
          - "mkdir -p .github/workflows"
          - "python scripts/generate_ci.py --output .github/workflows/golden-testset-ci.yaml"
          - "yamllint .github/workflows/golden-testset-ci.yaml || true"
          - "act -j ci --dryrun || echo 'act not installed, skipping local test'"
        verify:
          - "python scripts/validate_ci.py --file .github/workflows/golden-testset-ci.yaml"
      - id: p7.prefect
        name: "Prefect flow generation"
        run:
          - "mkdir -p flows"
          - "python scripts/generate_prefect_flow.py --output flows/golden_testset_flow.py"
          - "ruff check flows/golden_testset_flow.py --fix"
          - "python -m flows.golden_testset_flow --dry-run || true"
        verify:
          - "python scripts/validate_flow.py --file flows/golden_testset_flow.py"

  - id: phase8
    name: "Monitoring & Operations"
    depends_on: [phase7]
    tasks:
      - id: p8.dashboards
        name: "Monitoring dashboards"
        run:
          - "mkdir -p dashboards"
          - "python scripts/generate_sql_queries.py --output dashboards/golden_testset_metrics.sql"
          - "python scripts/generate_grafana_dashboard.py --output dashboards/golden_testset.json || true"
        verify:
          - "psql \"$DATABASE_URL\" -f dashboards/golden_testset_metrics.sql --single-transaction"
      - id: p8.alerts
        name: "Configure alerts"
        run:
          - "python scripts/generate_alerts.py --output configs/alerts.yaml"
        alerts:
          - "quality_threshold < 0.7"
          - "daily_cost_budget > 10.00"
          - "upload_failures > 3"
          - "generation_duration > 300"
        verify:
          - "python scripts/test_alerts.py --all"
      - id: p8.docs
        name: "Operational documentation"
        run:
          - "python scripts/generate_ops_docs.py --output docs/monitoring_guide.md"
          - "python scripts/generate_troubleshooting.py --output docs/troubleshooting.md"
          - "python scripts/generate_runbook.py --output docs/runbook.md"
        verify:
          - "python scripts/validate_docs.py --dir docs/"

checkpoints:
  on_phase_complete:
    - "python scripts/release/update_progress.py --phase ${PHASE_ID}"
    - "python scripts/release/run_phase_tests.py --phase ${PHASE_ID}"
    - "git add -A && git commit -m 'Complete phase ${PHASE_ID}' || true"
    - "python scripts/release/tag_version.py --phase ${PHASE_ID}"
    - "python scripts/release/update_changelog.py --phase ${PHASE_ID}"
    - "python scripts/release/notify_slack.py --phase ${PHASE_ID} || true"

  on_failure:
    - "python scripts/release/capture_failure.py --phase ${PHASE_ID} --task ${TASK_ID}"
    - "python scripts/release/rollback_phase.py --phase ${PHASE_ID} --auto"
    - "python scripts/release/notify_oncall.py --severity high || true"

  on_success:
    - "python scripts/release/deploy_staging.py --component golden-testset"
    - "python scripts/release/run_smoke_tests.py --env staging"
    - "python scripts/release/generate_report.py --output reports/implementation_report.md"

references:
  documentation:
    - name: "Implementation Plan"
      path: "docs/golden_testset_management_plan.md"
    - name: "Quality Gates"
      note: "P95 < 2s, coverage ≥ 90%, diversity ≥ 0.7, upload < 30s"
    - name: "Migration Guide"
      path: "docs/migration_guide.md"

  external:
    - name: "RAGAS Documentation"
      url: "https://docs.ragas.io/"
    - name: "Phoenix Documentation"
      url: "https://docs.arize.com/phoenix/"
    - name: "PostgreSQL Audit"
      url: "https://wiki.postgresql.org/wiki/Audit_trigger"
    - name: "Semantic Versioning"
      url: "https://semver.org/"

audit:
  provenance:
    - "All testset versions immutable after creation"
    - "Complete audit trail in golden_testset_audit_log"
    - "Source document hash tracked in metadata"
    - "Generation parameters preserved"

  compliance:
    - "GDPR: No PII in testsets"
    - "SOC2: Access logging enabled"
    - "ISO27001: Encryption at rest"