

# The Collaborative Dyad: A Comprehensive Literature Review on User Behavior, Intent, and Adoption in Human-LLM Interaction

## Introduction

### The Paradigm Shift from HCI to HAI

The proliferation of Large Language Models (LLMs) represents a fundamental paradigm shift in human-computer interaction (HCI). Historically, HCI has been predicated on users interacting with deterministic systems, where inputs map to predictable outputs through graphical user interfaces or command-line instructions. The advent of LLMs has transformed this dynamic, recasting the computer from a passive tool into an active, probabilistic partner in communication, creation, and cognition.1 This evolution necessitates a dedicated field of study—Human-AI Interaction (HAI)—focused on the complex sociotechnical dynamics of this new collaborative dyad. Unlike traditional software, LLMs exhibit emergent behaviors, engage in natural language dialogue, and generate novel content, making their interactions fluid, conversational, and often unpredictable.3 This shift demands new frameworks for understanding, designing, and evaluating the collaboration between human and artificial intelligence.

### Defining the Scope: User Behavior, Intent, and Adoption

This review synthesizes the burgeoning academic literature on human-LLM interaction by analyzing it through three core pillars that are essential for understanding and shaping this new paradigm:

* User Behavior: This dimension encompasses the observable actions, cognitive processes, decision-making heuristics, and mental models users employ when interacting with LLMs. It moves beyond simple usage metrics to investigate *how* and *why* users act in certain ways, from their patterns of prompting and refinement to their strategies for verifying AI-generated content.5  
* User Intent: This pillar focuses on the "gulf of execution"—the process by which a user's high-level goals and motivations are translated into machine-interpretable instructions, or prompts. It examines the challenges users face in articulating their needs effectively and the capacity of LLMs to accurately infer these underlying intents from often ambiguous natural language inputs.8  
* Adoption: This dimension explores the critical factors that govern the sustained and effective integration of LLMs into human workflows. It centers on the dynamics of user trust, the calibration of reliance, and the ultimate performance of the human-AI team, recognizing that successful adoption depends on a delicate balance between leveraging AI capabilities and mitigating its inherent risks.11

### Central Thesis and Report Structure

The central argument of this review is that achieving effective and responsible human-LLM collaboration is a complex sociotechnical challenge that cannot be solved by technological scaling alone. Instead, it hinges on the deliberate, human-centered design of systems that account for the cognitive biases, limitations, and adaptive strategies of their human users. This report will build this argument by systematically reviewing the literature across several key areas. Section 2 explores the cognitive landscape of trust and reliance that underpins all human-AI decision-making. Section 3 deconstructs the active process of user engagement, from the formulation of intent to the patterns of interaction. Section 4 examines empirical findings from the application of LLMs in specific, high-impact domains. Section 5 synthesizes actionable design principles and mitigation strategies for fostering safer and more effective partnerships. Finally, Section 6 looks toward the future, identifying emerging frontiers and critical research gaps that will define the next phase of human-AI interaction.

## The Cognitive Landscape of Human-AI Collaboration: Trust, Reliance, and Decision-Making

The efficacy of any human-AI partnership is fundamentally governed by a set of core psychological constructs that dictate how users perceive, evaluate, and ultimately act upon AI-generated outputs. This section deconstructs these foundational factors, exploring the concepts of trust and reliance, the quantitative models used to predict user behavior, and the profound influence of the surrounding context on decision-making.

### Defining the Core Constructs: Trust, Reliance, and Calibration

A central theme in the HAI literature is the crucial distinction between maximizing user trust and fostering appropriate reliance. The ultimate goal of human-centered AI design is not to make users trust the system unconditionally, but to empower them to calibrate their reliance accurately—accepting correct AI outputs while rejecting incorrect ones.11 This calibrated state is the cornerstone of effective human-AI team performance, as both excessive and insufficient trust can be equally detrimental to outcomes.13

The challenge of inappropriate reliance manifests in two primary forms. The first, overreliance, also known as automation bias, is the tendency for users to accept incorrect AI recommendations uncritically.11 This behavior is often driven by an overestimation of the AI's capabilities, a desire to minimize cognitive effort, or the sheer plausibility of the AI's output. The very nature of generative AI exacerbates this problem. Unlike traditional systems that might produce a clearly erroneous number, LLMs generate fluent, grammatically perfect, and highly convincing prose, which significantly increases the cognitive burden of verification and makes errors far more difficult to detect.15 This creates a "plausibility trap," where the model's greatest strength—its linguistic prowess—becomes a primary vector for user error. Even skilled professionals are susceptible, especially under time pressure or without adequate training on the system's limitations.15

The second form of inappropriate reliance is under-reliance, or algorithmic aversion. This is the tendency for users to reject correct and beneficial AI advice, often after witnessing the system make a mistake.13 This behavior can lead users to perform worse than they would have with AI assistance, thereby negating the system's potential benefits. Both over- and under-reliance stem from a failure of

calibrated trust, which is the ideal state where a user's confidence in the AI is dynamic and proportional to its demonstrated performance within a specific context. The literature consistently shows that users struggle to achieve this calibration because they often lack a clear mental model of what the AI can do, how well it performs its functions, and the underlying mechanisms of its operation.11

### Modeling and Predicting Human Reliance in AI-Assisted Decision-Making

To move beyond descriptive accounts and toward predictive understanding, researchers have developed quantitative models of human reliance. Early approaches often assumed that reliance is the result of a rational, analytical process, akin to a cost-benefit analysis.17 However, more recent work argues that this view is incomplete and that it is critical to capture the

*affective process* that underlies the human-AI interaction.

In a significant advancement, Li, Lu, and Yin proposed a Hidden Markov Model to formalize this process.17 This model characterizes the interaction as a sequence of events where a user's latent (unobservable) trust in the AI evolves based on their experiences, such as instances of agreement or disagreement with the AI's recommendations. The user's decision to rely on the AI at any given moment is then a probabilistic function of this hidden trust state. Evaluations on human-subject data demonstrate that this Markovian approach significantly outperforms various baseline models in accurately predicting reliance behavior, providing a powerful quantitative framework for understanding the fluid dynamics of trust.17

This modeling becomes particularly important in real-world scenarios where explicit performance feedback is limited or absent. Research by Lu and Yin shows that in such environments, users develop cognitive shortcuts or heuristics to gauge the AI's reliability.5 A key heuristic is using the level of human-AI agreement on tasks where the human is highly confident as a proxy for the AI's overall performance. While this can be an efficient strategy, it also creates a vulnerability; an adversary could strategically manipulate the AI's recommendations on these high-confidence tasks to artificially inflate or deflate a user's trust.5 Building on these findings, researchers have sought to develop unified frameworks that can "decode AI's nudge" by integrating these cognitive heuristics and contextual factors to generate more robust predictions of human behavior in AI-assisted decision-making.18

### The Influence of Context on Reliance and Trust

The decision to rely on an AI does not occur in a vacuum. It is profoundly shaped by the sociotechnical context in which the interaction is embedded. An individual's cognitive state and direct experience with an AI are heavily mediated by external factors, revealing reliance to be a sociotechnical phenomenon, not merely a cognitive one.

One of the most powerful contextual factors is social influence. Experimental work by Lu and colleagues demonstrates that users rely on AI-based credibility indicators significantly *more* when they are under the influence of their peers, even when the AI's advice is incorrect.5 This finding is critical, as it shows that social proof can override a user's direct assessment of an AI's performance. The dynamic becomes even more nuanced when social influence comes from multiple sources, such as layperson peers and credentialed experts. In these scenarios, the effect of an AI indicator is moderated by its level of agreement with the expert's judgment, suggesting that users engage in a complex process of weighing and integrating social and algorithmic signals.18 This implies that interventions aimed at fostering appropriate reliance cannot be confined to the user-AI interface alone; they must account for the broader information ecosystem and the social dynamics at play.

Other contextual factors also play a significant role. The stakes of the decision and the type of task influence reliance patterns.11 A user might be more willing to trust an LLM for a low-stakes creative writing task than for a high-stakes medical diagnosis. Similarly, reliance behaviors differ between objective tasks with clear right-or-wrong answers and subjective tasks that involve matters of opinion or taste.11 Finally,

group dynamics add another layer of complexity. Research comparing the performance of individuals versus groups in AI-assisted tasks, such as recidivism risk assessment, indicates that the collaborative process within a human team further mediates how AI advice is interpreted and used.18

| Category | Factor | Description | Effect on Reliance | Supporting Snippets |
| :---- | :---- | :---- | :---- | :---- |
| Human Factors | Domain Expertise | User's knowledge of the task domain. | Higher expertise can decrease overreliance but may also lead to under-reliance. | 12 |
|  | AI Literacy | User's understanding of how AI/LLMs work. | Low AI literacy correlates with higher overreliance, especially when explanations are present. | 11 |
|  | Cognitive Biases | Inherent biases like automation bias and confirmation bias. | Increases overreliance by predisposing users to agree with the AI. | 13 |
|  | Self-Confidence | User's confidence in their own abilities. | Higher self-confidence is associated with more critical thinking and less overreliance. | 21 |
|  | Demographics | Factors like age and gender. | Some studies show gender differences in trust and decision latency. | 22 |
| AI System Factors | Perceived Accuracy | The user's belief in the AI's correctness, based on past performance. | Higher perceived accuracy directly increases reliance. | 5 |
|  | Explanation Type | The style and content of explanations provided. | Persuasive explanations can increase overreliance; verification-focused ones can decrease it. | 12 |
|  | Uncertainty Expression | Whether the AI explicitly signals its uncertainty. | Explicit uncertainty reduces overreliance by prompting user verification. | 12 |
|  | Fluency & Plausibility | The linguistic quality of the generated output. | High fluency increases overreliance by making outputs seem more credible and harder to critique. | 12 |
|  | Declared Identity | Whether the AI is presented as human, a simple AI, or a complex LLM. | Declared identity significantly affects cooperation rates and trust. | 22 |
| Task/Context Factors | Task Difficulty/Stakes | The complexity and consequences of the decision. | Higher stakes can increase scrutiny but also reliance if the user feels overwhelmed. | 11 |
|  | Social Influence | The presence of opinions from peers or experts. | Social influence generally increases reliance on AI recommendations. | 18 |
|  | Time Pressure | The amount of time available for the user to make a decision. | High time pressure increases overreliance as users take cognitive shortcuts. | 15 |
|  | Emotional Context | Whether the interaction is emotionally neutral or charged. | High-emotion contexts can trigger negative reactions to AI's human-like language. | 25 |
|  |  |  |  |  |

## Deconstructing User Engagement: From Latent Intent to Observable Interaction

While the previous section focused on the user's internal cognitive state of trust and reliance, this section examines their active role in shaping the interaction. It analyzes the process of translating a high-level goal into an effective prompt, the distinct behaviors of novice and expert users, and frameworks for modeling the granular patterns of interaction that constitute AI-assisted work.

### The Challenge of User Intent: Bridging the "Gulf of Execution"

Effective human-LLM interaction begins with the successful communication of user intent. To systematically study this process, researchers have moved to develop formal classifications of the goals users bring to these systems. These taxonomies of user intent go beyond the traditional "navigational, informational, transactional" model used for web search, reflecting the broader capabilities of generative models.8 Common intent categories identified in the literature include information seeking, content generation, brainstorming, editing and refinement, summarization, instruction following, and open-ended conversation.8

Despite the development of these taxonomies, a primary source of user dissatisfaction is the difficulty of intent recognition, where LLMs frequently fail to accurately infer the user's goal from an ambiguous prompt.9 Empirical studies comparing different model versions show that while newer models like GPT-4 are generally better at recognizing common intents, they can paradoxically be outperformed by older models on less frequent or more nuanced intents.26

This challenge leads to a significant and counter-intuitive finding in the literature, which can be described as a "paradox of explicitness." While the failure of novices to craft clear prompts suggests that making prompts more explicit should improve outcomes, studies on intent-based prompt reformulation show the opposite. In these studies, a user's ambiguous prompt is first classified to identify the latent intent, and then automatically rewritten into a more explicit, structured prompt. Surprisingly, users are often *less* satisfied with the LLM's response to the "improved," reformulated prompt than they were with the response to their original, less-perfect query.9 This paradox suggests that the process of automatic reformulation may strip the prompt of subtle but important context that the user had implicitly encoded. It also points to the value users place on agency and the feeling of a collaborative dialogue; an overly mechanized prompt-response cycle may feel less satisfying even if it is technically more precise. The implication is that the most effective support tools may not be those that reformulate prompts

*for* the user, but those that guide the user to reformulate prompts *themselves*, preserving their sense of control and authorship.

### Prompting as a Skill: The Novice-Expert Gap

The ability to craft effective prompts—often termed prompt engineering—has emerged as a critical skill for leveraging LLMs. The seminal study "Why Johnny Can't Prompt" by Zamfirescu-Pereira and colleagues provides a rich, qualitative description of the specific ways in which non-AI experts struggle with this task.27

The core of the problem lies in mismatched mental models. Novice users intuitively approach prompting as if they are instructing another human. They use natural language with the expectation of human-like inference, common sense, and shared context. Consequently, they are often confused and frustrated when small, semantically trivial changes to a prompt lead to vastly different outputs, or when the LLM fails to grasp implicit instructions.28 This mismatch leads to a characteristic pattern of

opportunistic versus systematic exploration. Instead of methodically testing a prompt strategy across different contexts to ensure its robustness, novices tend to engage in ad-hoc trial and error. They often over-generalize from a single successful or failed interaction, either abandoning a promising approach prematurely or declaring "premature victory" without verifying that the desired behavior is consistent.28

These findings underscore the urgent need for improved "LLM literacy" among the general public and for the development of better design tools.28 Such tools could help bridge the novice-expert gap by encouraging more systematic evaluation and helping users build more accurate mental models of the LLM's capabilities and limitations. A complementary approach is the development of

co-audit tools, which shift the focus from perfecting the input prompt to helping the user more effectively scrutinize and verify the AI-generated output.31

### Modeling and Measuring Interaction Patterns

To understand the impact of LLMs on workflows, it is necessary to move beyond high-level outcomes and model the fine-grained patterns of user activity. The CUPS (Coding, Understanding, and Prompting States) framework, developed by Mozannar and colleagues for studying AI-assisted programming, provides a powerful methodology for this kind of analysis.6 By having developers retrospectively label video recordings of their coding sessions, the researchers created a detailed taxonomy of cognitive and behavioral states.

This framework reveals that a significant portion of a developer's time is spent on activities that are often invisible to traditional productivity metrics. While metrics like "acceptance rate" of AI suggestions are easily measured, the CUPS model shows that the most time-consuming states are often the "in-between" activities of *Verifying Suggestion*, *Prompt Crafting*, and *Editing Last Suggestion*.7 This analysis leads to a profound conclusion: the cognitive overhead and time cost of interacting with an AI code assistant are far greater than commonly assumed, nearly doubling the estimate of how much developer time is attributable to the system.7 This suggests that interaction itself is the "dark matter" of AI-assisted work—a vast and costly component of the workflow that is largely unmeasured.

The value of this model extends beyond measurement. The research also demonstrates that it is feasible, though still challenging, to predict a user's current CUPS state from real-time telemetry data, such as typing speed, pause duration, and features of the AI's suggestion.6 While the accuracy of these predictive models requires further improvement, they open the possibility of creating adaptive interfaces that can provide context-sensitive interventions. For example, if the system detects that a user is spending an unusually long time in the "Verifying Suggestion" state, it could proactively offer debugging tools or links to relevant documentation. Such advancements would reframe the goal of HAI design away from simply generating better initial suggestions and toward optimizing the efficiency and reducing the cognitive friction of the entire interactive workflow.

| Intent Category | Description | Example Prompt | Key Challenges & Relevant Snippets |
| :---- | :---- | :---- | :---- |
| Information Seeking | Using the LLM as an answer engine to find factual information or explanations. | "Explain the theory of relativity in simple terms." | Hallucinations, lack of citations, user overreliance on plausibility. |
| Content Generation | Creating new, original text for a specific purpose (e.g., creative writing, marketing copy, email drafts). | "Write a short poem about autumn." | Maintaining coherence, avoiding repetition, capturing specific tone/style, potential for bias. |
| Brainstorming & Ideation | Using the LLM as a creative partner to generate a wide range of ideas or explore different angles on a topic. | "Give me 10 blog post ideas about sustainable travel." | Diversity of ideas, avoiding generic suggestions, building on user input effectively. |
| Editing & Refinement | Improving existing text by correcting grammar, rephrasing for clarity, changing tone, or shortening/lengthening. | "Make this sentence sound more professional: 'I think we should do this.'" | Preserving original meaning, understanding subtle nuances, avoiding over-correction. |
| Summarization | Condensing a long piece of text into a shorter, coherent summary. | "Summarize the following article about climate change." | Identifying key points vs. peripheral details, accuracy, avoiding misinterpretation. |
| Instruction Following / Task Execution | Providing a set of explicit steps or rules for the LLM to follow to complete a task. | "Translate this text to French, then format it as a JSON object with keys 'original' and 'translated'." | Brittleness, failure to follow complex or multi-step instructions, misinterpreting constraints. |
| Conversational Partner | Engaging in open-ended dialogue for companionship, entertainment, or therapeutic purposes. | "I had a really tough day today." | Maintaining persona consistency, showing empathy, avoiding harmful or inappropriate responses. |
|  |  |  |  |

## Paradigms of LLM-Assisted Workflows and Applications

Having established the general cognitive and behavioral principles of human-LLM interaction, this section examines how these dynamics play out in specific, high-impact application domains. Empirical studies across programming, writing, decision-making, and scientific research reveal common patterns of adoption, behavioral change, and the persistent gap between the potential of LLMs and their reliable application in real-world contexts.

### AI-Assisted Programming and Software Engineering

The integration of LLMs into software development workflows, most notably through tools like GitHub Copilot, has been a major area of study. These tools are intended to accelerate the efforts of software engineers and improve their productivity.7 However, their adoption introduces a new set of behaviors and risks. The CUPS model reveals that developers' workflows shift significantly, with a large amount of time dedicated to verifying, editing, and debugging AI-generated code suggestions.6 This highlights a fundamental change in the developer's role from a primary author of code to a curator and validator of AI-generated code.

This shift carries significant risks. Overreliance on AI-generated code without proper verification can lead to the deployment of applications with critical security vulnerabilities or subtle bugs.15 Furthermore, engineers report that one of the largest challenges they face is controlling the context of the interaction—ensuring the AI has the right information to generate a relevant and correct suggestion.38 The need for better evaluation methodologies that go beyond simple code acceptance rates and capture the complex, collaborative nature of modern programming is a recurring theme, as exemplified by initiatives like the "RealHumanEval" study.39

### Collaborative Writing and Content Creation

LLMs are increasingly being deployed as writing assistants in both professional and academic contexts. Applications range from "Corporate Communication Companions" designed to help employees with workplace social media posts to tools that assist with the entire academic writing process.18 The design of high-quality, domain-specific datasets of human-AI collaborative writing is a crucial area of research for exploring and improving these capabilities.33

The primary benefit of these tools is their ability to shift cognitive effort. By automating lower-level writing processes such as grammar correction, summarization, and initial drafting, LLMs can free up a writer's cognitive resources to focus on higher-order aspects of composition, such as argument development, structuring, and creative expression.34 Studies show that users who engage in more iterative and highly interactive processes with these tools tend to achieve better performance on writing tasks.34

However, this offloading of cognitive effort is not without its costs. A survey of knowledge workers using generative AI revealed self-reported reductions in the cognitive effort they apply to their tasks.21 While this can be perceived as an efficiency gain, it raises significant concerns about the potential for long-term skill atrophy and a decline in critical thinking. The nature of critical engagement with text appears to be changing: for many users, the primary critical task is no longer the initial generation of ideas but the subsequent verification of AI-generated facts and the integration of AI outputs into a coherent whole.21 This pattern suggests the emergence of a "curation economy" in knowledge work, where the most valuable human skill is not creation from scratch, but the critical judgment required to effectively select, validate, and synthesize AI-generated content.

### High-Stakes AI-Assisted Decision-Making

The application of LLMs to high-stakes decision-making in fields like medicine, law, and finance presents both immense opportunities and significant risks. In these domains, the "reality gap"—the difference between an LLM's performance on sanitized benchmarks and its reliability in messy, real-world contexts—becomes particularly acute.

In group settings, LLMs can be used constructively to enhance decision quality. For instance, an LLM can be tasked with playing the role of a "devil's advocate," systematically challenging a team's assumptions and surfacing alternative viewpoints that might otherwise be overlooked.18

In critical clinical workflows, however, the challenges are formidable. One study on LLM-assisted brain MRI differential diagnosis found that while the human-AI team could achieve superior accuracy compared to a human using conventional internet search, this potential was frequently undermined by interaction failures. These failures included inaccurate or incomplete case descriptions in the user's prompt, factual "hallucinations" by the LLM, and the user's failure to sufficiently contextualize the LLM's generic response within the specific details of the patient's case.41 This underscores a key principle for high-stakes domains: for AI decision support to be effective, it must be seamlessly integrated and made "unremarkable," fitting into existing critical processes without causing disruption.33

The dual nature of LLMs is also evident in the context of misinformation. While LLMs can be used to generate and spread convincing misinformation, they can also be deployed as tools to combat it. Studies have examined the use of AI-based credibility indicators to help users detect false information. However, the effectiveness of these indicators is heavily moderated by social influence, demonstrating again that technical solutions must be designed with a deep understanding of their sociotechnical context.5

### LLMs as a Research Tool: Synthetic Data Generation

Beyond assisting end-users, LLMs are also being explored as tools for researchers and developers, primarily for the task of synthetic data generation. The ability to generate large, labeled datasets can be invaluable in situations where real-world data is scarce, expensive to annotate, or protected by privacy regulations.42

However, the utility of this synthetically generated data has significant limitations. A key finding is that its effectiveness is negatively correlated with the subjectivity of the task. For classification tasks that are highly objective (e.g., spam email detection, news topic classification), models trained on LLM-generated data can perform nearly as well as those trained on real-world data. But for tasks that are highly subjective and rely on nuanced human judgment (e.g., sentiment analysis, hate speech detection), the performance of models trained on synthetic data drops off dramatically.42 This suggests that current LLMs struggle to capture the rich diversity and subtle complexities inherent in subjective human expression.

The methodology of data generation also matters. Guiding the LLM with a small number of real-world examples (few-shot prompting) produces significantly higher-quality synthetic data than generating it from a textual prompt alone (zero-shot prompting).42 This indicates that LLMs are currently more effective as data augmenters—expanding upon an existing seed of real data—than as pure data creators in complex domains.

## Designing for Effective Partnership: Principles and Mitigation Strategies

The challenges and user behaviors identified in the preceding sections underscore the need for a deliberate and principled approach to the design of human-LLM interaction. The literature offers a rich set of guidelines, strategies, and emerging design paradigms aimed at mitigating the risks of inappropriate reliance and fostering a safer, more productive human-AI partnership. These approaches range from foundational, universally applicable principles to novel interventions designed to actively promote critical thinking.

### Foundational Design Principles for Human-AI Interaction

A significant body of work has sought to synthesize decades of research into actionable guidelines for practitioners. The Guidelines for Human-AI Interaction, developed by researchers at Microsoft, represent one of the most comprehensive efforts in this area.47 This framework proposes 18 guidelines that cover the entire lifecycle of an interaction, grouped into four categories: Initially, During Interaction, When Wrong, and Over Time. Key principles include G1:

*Make clear what the system can do*, which addresses the need to set accurate user expectations from the outset; G4: *Show contextually relevant information*, which helps the user understand the basis for the AI's output; and G12: *Support efficient correction*, which acknowledges that AI errors are inevitable and that users must have easy ways to override or fix them.

These specific guidelines are part of a broader movement toward Human-Centered AI (HCAI). HCAI is a design philosophy that prioritizes human needs, values, and capabilities, with the goal of creating AI systems that augment and empower users rather than displacing or de-skilling them.50 This approach advocates for interdisciplinary collaboration, bringing together computer scientists with psychologists, ethicists, and domain experts to ensure that systems are transparent, fair, and inclusive.51

### Strategies for Communicating Uncertainty and Calibrating Trust

Given that overreliance is a primary risk, many design strategies focus on helping users better calibrate their trust by making the AI's limitations more salient. One common approach is to provide explanations for AI outputs. However, the literature reveals a complex "transparency-reliance" dilemma: more explanation does not always lead to better user decisions. While the intuitive assumption is that explaining an AI's reasoning will improve trust calibration, empirical studies show that some types of explanations can inadvertently *increase* overreliance.12 For instance, users with low AI literacy may misinterpret technical-sounding or numeric explanations as a sign of objective truth, while users with high literacy may be lulled into a false sense of security about their ability to debug the system.11 The effectiveness of an explanation hinges on its function. Explanations designed to persuade a user of the AI's correctness can be dangerous, whereas

verification-focused explanations—those that provide evidence, cite sources, or otherwise lower the user's cost of checking the work—are more effective at reducing overreliance.12 The guiding design principle should therefore be not simply to "explain more," but to "explain in a way that empowers the user to challenge the AI."

A more direct and often more effective strategy is to have the AI explicitly express its uncertainty. This can be achieved through various means, such as using first-person linguistic hedges ("I'm not sure, but...") 24, displaying numerical confidence scores alongside recommendations, or visually highlighting words or phrases in a generated text for which the model has low confidence.32 Multiple studies have demonstrated that these forms of uncertainty communication are highly effective, substantially increasing the rate at which users identify and correct incorrect information in AI outputs.32

### Interventions to Promote Critical Thinking: Cognitive Forcing and Frictional Design

Recognizing that users often default to fast, intuitive, and low-effort "System 1" thinking, a more assertive class of interventions aims to actively nudge them into a more slow, deliberative, and analytical "System 2" mindset. These interventions are often referred to as Cognitive Forcing Functions (CFFs).12 Examples include:

* AI Self-Critiques: The system is prompted to generate not only an answer but also a critique of that answer, pointing out potential weaknesses, alternative perspectives, or missing information. This explicitly models critical thinking for the user.12  
* Introducing Second Opinions: The interface provides the user with an independent second opinion, either from another AI model or from a repository of human expert decisions. This intervention has been shown to be highly effective at reducing overreliance by forcing the user to compare and reconcile conflicting advice.5

These interventions are part of a nascent but important design paradigm that can be termed "Frictional AI".53 This paradigm represents a radical departure from traditional HCI, which has long prioritized seamlessness, efficiency, and the reduction of cognitive load. Frictional AI argues that for generative AI, a certain amount of "programmed inefficiency" or "cognitive friction" is not only desirable but necessary for safety and effectiveness. By intentionally slowing down the interaction, these designs aim to prevent the uncritical acceptance of AI outputs and promote more thoughtful engagement.

A prime example of a frictional design pattern is "Judicial AI." Instead of providing a single, definitive answer, a Judicial AI system presents the user with well-reasoned arguments for multiple, competing hypotheses or outcomes.56 For instance, in a medical diagnosis task, it might provide the evidence supporting Disease A, and then separately provide the evidence supporting a different diagnosis, Disease B. This forces the user to move from the role of a passive recipient of an answer to an active judge who must weigh the evidence. This approach is explicitly designed to mitigate automation bias and preserve the user's sense of agency and responsibility.56 This shift in design philosophy—from AI as an assistant to AI as a dialectical sparring partner—may be one of the most important frontiers in creating truly collaborative and responsible human-AI systems.

| Underlying Mechanism | Strategy | Description | Example Implementation | Supporting Snippets |
| :---- | :---- | :---- | :---- | :---- |
| Improving User's Mental Model | Transparency of Capabilities | Clearly communicate what the AI can and cannot do, including its limitations and error boundaries. | Onboarding tutorials; explicit statements like "I am an AI and can make mistakes." | 12 |
|  | Uncertainty Highlighting | Visually or textually signal the AI's confidence in its output. | Highlighting low-confidence words in a summary; providing a confidence score with a prediction. | 12 |
|  | Verification-Focused Explanations | Provide explanations that help the user check the AI's work, rather than just persuade them it's correct. | Citing sources for factual claims; showing the data points that most influenced a classification. | 12 |
| Forcing Deliberation (System 2 Thinking) | Cognitive Forcing Functions (CFFs) | Interventions designed to interrupt automatic acceptance and encourage critical thought. | Requiring a user to type a justification before accepting a high-stakes recommendation. | 12 |
|  | AI Self-Critique | Prompting the AI to generate potential flaws or alternative perspectives on its own output. | "Here is my answer. A potential weakness is that I did not consider the economic impact." | 12 |
|  | Introducing Second Opinions | Presenting an independent opinion from another source to encourage comparison and reduce reliance on a single AI. | An "Ask another AI" button; showing a human expert's prior decision on a similar case. | 18 |
| Structuring a Dialectical Process | Frictional Design | Intentionally adding "programmed inefficiencies" to slow down interaction and promote reflection. | Introducing a mandatory time delay before a critical AI-suggested action can be confirmed. | 53 |
|  | Devil's Advocate | The AI is programmed to actively challenge the user's or another AI's assumptions. | In a group meeting, an LLM-powered agent is assigned the role of finding flaws in the proposed plan. | 18 |
|  | Judicial AI | The AI presents well-reasoned arguments for multiple, competing hypotheses or outcomes. | For a medical diagnosis, the AI provides evidence for Disease A and counter-evidence, while also providing evidence for Disease B. | 56 |
|  |  |  |  |  |

## Emerging Frontiers and Future Research Trajectories

As the capabilities of LLMs continue to advance at a rapid pace, the frontiers of human-AI interaction are constantly expanding. The research community is now looking beyond the immediate challenges of single-user, single-task interactions to consider more complex, long-term, and systemic questions about the future of AI-mediated work and society. This section synthesizes discussions from recent workshops, position papers, and forward-looking research to identify these emerging frontiers.

### The Evolution from Assistants to Autonomous Agents

The dominant interaction paradigm is shifting from LLMs as assistive tools to agentic AI systems. These are systems capable of performing complex, multi-step tasks with a greater degree of autonomy, such as planning, tool use, and even collaborating with other AI agents.57 Frameworks like Microsoft's AutoGen are being developed to enable the creation of sophisticated multi-agent conversational systems that can tackle complex problems collaboratively.57

This evolution from assistant to agent has profound implications for human interaction. It raises new and urgent questions about human oversight, control handoffs, and shared accountability.60 The interaction model is transforming from one where a human directly instructs a tool to one where a human collaborates with, delegates to, and supervises a quasi-autonomous partner.22 Looking further ahead, interfaces may need to be designed with a dual audience in mind: they must be intuitive for humans while also being structured and machine-readable for other

AI agents that may interact with them on a user's behalf.61

### The Future of Human-Centered AI (HCAI) and Interaction Design

The academic and industrial research communities are actively working to shape a human-centered future for AI. Major conferences in HCI and AI, such as CHI, CSCW, and AAAI, are now regular venues for workshops and panels dedicated to defining the future of HAI.47 A central theme of these discussions is the need to design AI that works

*with* people to augment their capabilities, rather than simply working *for* them as a means of automation.62

This research is being led by a number of key institutional hubs, including university centers like Stanford's Institute for Human-Centered AI (HAI) and Carnegie Mellon's Human-Computer Interaction Institute (HCII), as well as dedicated teams within major corporate labs, such as Microsoft's Human-AI eXperiences (HAX) team, Google's People \+ AI Research (PAIR) initiative, Meta's Reality Labs, and IBM's Human-Centered AI group.52 These groups champion interdisciplinary approaches and are pushing the boundaries of interaction design. The future of the AI interface is envisioned as moving beyond the simple chatbot paradigm toward more

integrated and multimodal experiences: AI that is seamlessly embedded in existing workflows, hybrid interfaces that can fluidly switch between conversational and structured interaction, and multimodal systems that can understand and generate content across text, voice, visuals, and gestures.58

### Long-Term Societal and Cognitive Impacts

As generative AI becomes more deeply integrated into daily life, researchers are beginning to grapple with its potential long-term consequences for human cognition and society. A primary concern is the risk of skill atrophy and cognitive offloading. The "irony of automation," first described by Bainbridge, posits that by automating routine tasks, a system deprives its human user of the regular practice needed to maintain their skills, leaving them unprepared to handle exceptions or situations where the automation fails.21 There is growing concern that over-dependence on LLMs for tasks like writing, problem-solving, and information synthesis will erode users' abilities to think critically and act independently.15

Another critical frontier is the need for more research into demographic and cultural considerations. Early studies are beginning to explore how factors like gender, age, and expertise level influence trust, reliance, and interaction patterns.22 For example, one study found significant gender differences in decision latency and cooperative attitudes toward different types of AI agents.22 Furthermore, as AI systems are deployed globally, designing

culturally aware machines that can understand and respect local norms, values, and communication styles is a formidable but essential challenge for ensuring equitable and effective human-AI interaction.73

### Identified Research Gaps and Future Directions

This review of the literature reveals several critical gaps where future research is urgently needed:

* Longitudinal Studies: The vast majority of current research is based on short-term, lab-based user studies. While valuable, these studies cannot capture how user behavior, skills, trust, and reliance patterns evolve over weeks, months, or years of sustained, real-world interaction with LLMs. There is a pressing need for long-term, longitudinal research to understand the dynamics of adoption and the potential for long-term cognitive effects.  
* Complex Collaboration Scenarios: The literature is heavily skewed toward studying single-user, single-AI interactions.76 The future of work, however, is likely to involve more complex scenarios, such as multiple human users collaborating with a single AI, or single users interacting with teams of specialized AI agents. Research must begin to explore these more complex topologies of human-AI teaming.  
* Standardized Methodologies and Metrics: The field currently lacks standardized methods and metrics for evaluating human-AI collaboration. This makes it difficult to compare findings across studies and to build a cumulative body of knowledge. Future work should focus on developing and validating robust evaluation frameworks that capture not only task performance but also the quality of the interaction, the user's cognitive load, and other human-centered outcomes.60  
* The Impact of Demographics and Expertise: While some studies have begun to explore the role of user characteristics, this area remains significantly under-investigated. Systematic research is required to understand how demographic factors (e.g., age, culture, cognitive style) and varying levels of both domain expertise and AI literacy shape the full spectrum of human-LLM interaction, from prompting strategies to reliance behaviors.

## Synthesis and Concluding Remarks

This comprehensive review of the literature on human-LLM interaction reveals a field grappling with the profound complexities of a new technological paradigm. The research paints a clear picture of the LLM as a double-edged sword: its unprecedented fluency and generative power offer transformative potential, yet these same qualities create a "plausibility trap" that exacerbates the risks of user overreliance and complicates the task of critical verification. The decision to trust and rely on an LLM is not a simple cognitive calculation but a deeply sociotechnical phenomenon, shaped as much by the user's social context and individual biases as by the AI's objective performance.

Across diverse applications—from writing code and academic papers to making high-stakes medical diagnoses—a consistent behavioral shift is emerging. The user's primary role is evolving from that of a creator to a curator, where the most valuable human contribution is no longer the generation of initial content but the critical evaluation, refinement, and contextualization of AI-generated outputs. In response to the challenges this new dynamic presents, the field of interaction design is undergoing its own evolution. A new paradigm of "frictional" or "dialectical" design is emerging, one that deliberately challenges the long-held HCI goal of seamless efficiency. By intentionally introducing cognitive friction through interventions like second opinions and competing explanations, these new approaches aim to foster a more deliberative and reflective engagement, transforming the AI from a simple assistant into a partner that promotes better human thinking.

Ultimately, the literature makes a compelling case that effective and responsible human-LLM interaction is not an inevitable outcome of developing more powerful models. Rather, it will be the product of deliberate, human-centered design. Achieving a future of productive and safe human-AI partnership requires a deep, empirically-grounded understanding of user psychology, a willingness to prioritize critical thinking over frictionless convenience, and an unwavering commitment to the HCAI principle of building systems that augment, rather than automate, human intelligence. To this end, the research community must move to address the critical gaps identified in this review—particularly the need for more longitudinal, ecologically valid, and methodologically robust studies—to guide the co-evolution of humans and AI toward a beneficial and collaborative future.

#### Works cited

1. (PDF) Re-examining Whether, Why, and How Human-AI Interaction Is Uniquely Difficult to Design \- ResearchGate, accessed September 23, 2025, [https://www.researchgate.net/publication/338459008\_Re-examining\_Whether\_Why\_and\_How\_Human-AI\_Interaction\_Is\_Uniquely\_Difficult\_to\_Design](https://www.researchgate.net/publication/338459008_Re-examining_Whether_Why_and_How_Human-AI_Interaction_Is_Uniquely_Difficult_to_Design)  
2. Human-AI Interaction in the Age of Large Language Models, accessed September 23, 2025, [https://ojs.aaai.org/index.php/AAAI-SS/article/download/31183/33343/35239](https://ojs.aaai.org/index.php/AAAI-SS/article/download/31183/33343/35239)  
3. Human-AI Interaction with Large Language Models in Complex Information Tasks: Prompt Engineering Strategies \- ERIC, accessed September 23, 2025, [https://files.eric.ed.gov/fulltext/EJ1476226.pdf](https://files.eric.ed.gov/fulltext/EJ1476226.pdf)  
4. ChatGPT: The Future of Human-AI Interaction \- ThinkML, accessed September 23, 2025, [https://thinkml.ai/chatgpt-chatbot-complete-overview/](https://thinkml.ai/chatgpt-chatbot-complete-overview/)  
5. Accounting Individual Cognition and Social Influence for Better Human-AI Decision Making, accessed September 23, 2025, [https://hammer.purdue.edu/articles/thesis/\_b\_Accounting\_Individual\_Cognition\_and\_Social\_Influence\_for\_Better\_Human-AI\_Decision\_Making\_b\_/28905017](https://hammer.purdue.edu/articles/thesis/_b_Accounting_Individual_Cognition_and_Social_Influence_for_Better_Human-AI_Decision_Making_b_/28905017)  
6. Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming \- of Eric Horvitz, accessed September 23, 2025, [http://erichorvitz.com/reading\_between\_the\_lines\_supplemental.pdf](http://erichorvitz.com/reading_between_the_lines_supplemental.pdf)  
7. Reading Between the Lines: Modeling User ... \- DSpace@MIT, accessed September 23, 2025, [https://dspace.mit.edu/bitstream/handle/1721.1/155163/3613904.3641936.pdf?sequence=1\&isAllowed=y](https://dspace.mit.edu/bitstream/handle/1721.1/155163/3613904.3641936.pdf?sequence=1&isAllowed=y)  
8. Understanding User Experience in Large Language Model ... \- arXiv, accessed September 23, 2025, [https://arxiv.org/pdf/2401.08329](https://arxiv.org/pdf/2401.08329)  
9. \[Literature Review\] User Intent Recognition and Satisfaction with Large Language Models: A User Study with ChatGPT \- Moonlight, accessed September 23, 2025, [https://www.themoonlight.io/en/review/user-intent-recognition-and-satisfaction-with-large-language-models-a-user-study-with-chatgpt](https://www.themoonlight.io/en/review/user-intent-recognition-and-satisfaction-with-large-language-models-a-user-study-with-chatgpt)  
10. user intent recognition and satisfaction with large language models:auser study with chatgpt \- mediaTUM, accessed September 23, 2025, [https://mediatum.ub.tum.de/doc/1735613/lzs01scxrh9m12brkxpb03r1l.2402.02136.pdf](https://mediatum.ub.tum.de/doc/1735613/lzs01scxrh9m12brkxpb03r1l.2402.02136.pdf)  
11. www.microsoft.com, accessed September 23, 2025, [https://www.microsoft.com/en-us/research/wp-content/uploads/2022/06/Aether-Overreliance-on-AI-Review-Final-6.21.22.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2022/06/Aether-Overreliance-on-AI-Review-Final-6.21.22.pdf)  
12. Appropriate reliance on GenAI: \- Research synthesis \- Microsoft, accessed September 23, 2025, [https://www.microsoft.com/en-us/research/wp-content/uploads/2024/03/GenAI\_AppropriateReliance\_Published2024-3-21.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2024/03/GenAI_AppropriateReliance_Published2024-3-21.pdf)  
13. A Study of AI Strength and Weakness Transparency Communication on Human-AI Collaboration \- arXiv, accessed September 23, 2025, [https://arxiv.org/pdf/2508.09033](https://arxiv.org/pdf/2508.09033)  
14. Exploring automation bias in human–AI collaboration: a review and implications for explainable AI \- ResearchGate, accessed September 23, 2025, [https://www.researchgate.net/publication/392771285\_Exploring\_automation\_bias\_in\_human-AI\_collaboration\_a\_review\_and\_implications\_for\_explainable\_AI](https://www.researchgate.net/publication/392771285_Exploring_automation_bias_in_human-AI_collaboration_a_review_and_implications_for_explainable_AI)  
15. Measuring and mitigating overreliance is necessary for building human-compatible AI \- arXiv, accessed September 23, 2025, [https://www.arxiv.org/pdf/2509.08010](https://www.arxiv.org/pdf/2509.08010)  
16. Fostering appropriate reliance on AI \- Microsoft Research, accessed September 23, 2025, [https://www.microsoft.com/en-us/research/video/fostering-appropriate-reliance-on-ai/](https://www.microsoft.com/en-us/research/video/fostering-appropriate-reliance-on-ai/)  
17. Modeling Human Trust and Reliance in AI-Assisted Decision Making: A Markovian Approach | Proceedings of the AAAI Conference on Artificial Intelligence, accessed September 23, 2025, [https://ojs.aaai.org/index.php/AAAI/article/view/25748](https://ojs.aaai.org/index.php/AAAI/article/view/25748)  
18. ‪Zhuoran Lu‬ \- ‪Google Scholar‬, accessed September 23, 2025, [https://scholar.google.com/citations?user=nWHEpdcAAAAJ\&hl=en](https://scholar.google.com/citations?user=nWHEpdcAAAAJ&hl=en)  
19. (PDF) Human Reliance on Machine Learning Models When Performance Feedback is Limited: Heuristics and Risks \- ResearchGate, accessed September 23, 2025, [https://www.researchgate.net/publication/351419092\_Human\_Reliance\_on\_Machine\_Learning\_Models\_When\_Performance\_Feedback\_is\_Limited\_Heuristics\_and\_Risks](https://www.researchgate.net/publication/351419092_Human_Reliance_on_Machine_Learning_Models_When_Performance_Feedback_is_Limited_Heuristics_and_Risks)  
20. Zhuoran Lu \- Homepage, accessed September 23, 2025, [https://zhuoranlu.github.io/](https://zhuoranlu.github.io/)  
21. The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers \- Microsoft, accessed September 23, 2025, [https://www.microsoft.com/en-us/research/wp-content/uploads/2025/01/lee\_2025\_ai\_critical\_thinking\_survey.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/01/lee_2025_ai_critical_thinking_survey.pdf)  
22. When Trust Collides: Decoding Human-LLM Cooperation Dynamics through the Prisoner's Dilemma \- arXiv, accessed September 23, 2025, [https://arxiv.org/html/2503.07320v2](https://arxiv.org/html/2503.07320v2)  
23. ‪Martim Brandão‬ \- ‪Google Scholar‬, accessed September 23, 2025, [https://scholar.google.com/citations?user=zujQKTMAAAAJ\&hl=en](https://scholar.google.com/citations?user=zujQKTMAAAAJ&hl=en)  
24. ‪Mihaela Vorvoreanu, PhD‬ \- ‪Google Scholar‬, accessed September 23, 2025, [https://scholar.google.com/citations?user=IvlHCi0AAAAJ\&hl=en](https://scholar.google.com/citations?user=IvlHCi0AAAAJ&hl=en)  
25. Full article: When customers know it's AI: Experimental comparison of human and LLM-Based Communication in service recovery, accessed September 23, 2025, [https://www.tandfonline.com/doi/full/10.1080/13527266.2025.2540376?src=exp-la](https://www.tandfonline.com/doi/full/10.1080/13527266.2025.2540376?src=exp-la)  
26. User Intent Recognition and Satisfaction with Large Language Models: A User Study with ChatGPT \- arXiv, accessed September 23, 2025, [https://arxiv.org/html/2402.02136v2](https://arxiv.org/html/2402.02136v2)  
27. Online Appendix 1\_Prompting guidance\_online.xlsx, accessed September 23, 2025, [https://cdn.website-editor.net/s/82def2de8e954d20be13813905c24151/files/uploaded/White\_Paper\_Prompting\_guidance\_online.xlsx.pdf?Expires=1758952023\&Signature=ejpeGW\~xq9wxhCgeG07ZyxJfesORFb8FOKNklV7G9Nv\~U4A\~Rr0YTnV10w6GE5gePWZDp26fZc2Ehs0p1E137Kbz\~gsIsPBP2IQ6RBMQZSV-NxpvubR0myoMeehMTGjX1y3wdsDknppuHjZTpSsiVGWOLokVLJoCyciNbw\~2EqzcQiRlUW56q3Xh3b9Ij\~hU9SPQyjv-EAZ1d4LdaAyrNPQ-DdicpCtIHg8xeFyMRmNWip-ESp\~a2dF1EBwYDTcv82HPgNP7dn81P\~EIpzU0faVAWhgVSECxbrRtA5njQviwJuGHv0Th74BojyiBHdg6ldx23YE7lkGpdtoBrISYNQ\_\_\&Key-Pair-Id=K2NXBXLF010TJW](https://cdn.website-editor.net/s/82def2de8e954d20be13813905c24151/files/uploaded/White_Paper_Prompting_guidance_online.xlsx.pdf?Expires=1758952023&Signature=ejpeGW~xq9wxhCgeG07ZyxJfesORFb8FOKNklV7G9Nv~U4A~Rr0YTnV10w6GE5gePWZDp26fZc2Ehs0p1E137Kbz~gsIsPBP2IQ6RBMQZSV-NxpvubR0myoMeehMTGjX1y3wdsDknppuHjZTpSsiVGWOLokVLJoCyciNbw~2EqzcQiRlUW56q3Xh3b9Ij~hU9SPQyjv-EAZ1d4LdaAyrNPQ-DdicpCtIHg8xeFyMRmNWip-ESp~a2dF1EBwYDTcv82HPgNP7dn81P~EIpzU0faVAWhgVSECxbrRtA5njQviwJuGHv0Th74BojyiBHdg6ldx23YE7lkGpdtoBrISYNQ__&Key-Pair-Id=K2NXBXLF010TJW)  
28. Why Johnny Can't Prompt: How Non-AI Experts Try (and ... \- UMBC, accessed September 23, 2025, [https://courses.cs.umbc.edu/graduate/691/spring24/pdf/3544548.3581388.pdf](https://courses.cs.umbc.edu/graduate/691/spring24/pdf/3544548.3581388.pdf)  
29. Using Gen AI for Survey Research, accessed September 23, 2025, [https://wapor.org/wp-content/uploads/WAPOR-Feb-2025-Read-Only-1.pdf](https://wapor.org/wp-content/uploads/WAPOR-Feb-2025-Read-Only-1.pdf)  
30. Prompt Strategies in Lesson Plan Assessment: Insights from Pre-Service Teachers' Prompt Dataset \- Pixel International Conferences, accessed September 23, 2025, [https://conference.pixel-online.net/FOE/ICT4LL/files/foe/ed0015/PPT/8395-DEV7359-PPT-FOE15.pdf](https://conference.pixel-online.net/FOE/ICT4LL/files/foe/ed0015/PPT/8395-DEV7359-PPT-FOE15.pdf)  
31. Co-audit: tools to help humans double-check AI-generated content \- arXiv, accessed September 23, 2025, [https://arxiv.org/pdf/2310.01297](https://arxiv.org/pdf/2310.01297)  
32. Effects of LLM-based Search on Decision Making: Speed, Accuracy, and Overreliance \- Dan Goldstein, accessed September 23, 2025, [https://dangoldstein.com/papers/spatharioti\_rothschild\_goldstein\_hofman\_LLM\_Search\_CHI25.pdf](https://dangoldstein.com/papers/spatharioti_rothschild_goldstein_hofman_LLM_Search_CHI25.pdf)  
33. ‪Qian Yang‬ \- ‪Google Scholar‬, accessed September 23, 2025, [https://scholar.google.com/citations?user=yaSMILkAAAAJ\&hl=en](https://scholar.google.com/citations?user=yaSMILkAAAAJ&hl=en)  
34. Human-AI collaboration patterns in AI-assisted academic writing \- Taylor & Francis Online, accessed September 23, 2025, [https://www.tandfonline.com/doi/full/10.1080/03075079.2024.2323593](https://www.tandfonline.com/doi/full/10.1080/03075079.2024.2323593)  
35. Controlling Context: Generative AI at Work in Integrated Circuit Design and Other High-Precision Domains \- arXiv, accessed September 23, 2025, [https://arxiv.org/pdf/2506.14567](https://arxiv.org/pdf/2506.14567)  
36. ‪Hussein Mozannar‬ \- ‪Google Scholar‬, accessed September 23, 2025, [https://scholar.google.com/citations?user=XCfZyIkAAAAJ\&hl=en](https://scholar.google.com/citations?user=XCfZyIkAAAAJ&hl=en)  
37. ‪Lev Tankelevitch‬ \- ‪Google Scholar‬, accessed September 23, 2025, [https://scholar.google.com/citations?user=X21rO6QAAAAJ\&hl=en](https://scholar.google.com/citations?user=X21rO6QAAAAJ&hl=en)  
38. Human-AI collaboration in large language model-assisted brain MRI differential diagnosis: a usability study \- PMC, accessed September 23, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC12350562/](https://pmc.ncbi.nlm.nih.gov/articles/PMC12350562/)  
39. Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations \- ACL Anthology, accessed September 23, 2025, [https://aclanthology.org/2023.emnlp-main.647.pdf](https://aclanthology.org/2023.emnlp-main.647.pdf)  
40. Large language models for identifying social determinants of health \- PNAS, accessed September 23, 2025, [https://www.pnas.org/doi/pdf/10.1073/pnas.2501506122?download=true](https://www.pnas.org/doi/pdf/10.1073/pnas.2501506122?download=true)  
41. Choosing a Primary Care Physician — How AI Powered by Mind Genomics Thinking Provides Direction \- Medicon Publications, accessed September 23, 2025, [https://themedicon.com/pdf/medicalsciences/MCMS-08-264.pdf](https://themedicon.com/pdf/medicalsciences/MCMS-08-264.pdf)  
42. Deliverable 1.4 – Data Management Plan v2, accessed September 23, 2025, [https://cdn.prod.website-files.com/64b51b1700cf93214c4e50fd/66f3e3f3e07b27eb60e88744\_D1.4%20Data%20Management%20Plan\_Version%202.pdf](https://cdn.prod.website-files.com/64b51b1700cf93214c4e50fd/66f3e3f3e07b27eb60e88744_D1.4%20Data%20Management%20Plan_Version%202.pdf)  
43. Reply to Wang: Improving large language model approaches for identifying social determinants of health from clinical notes \- PNAS, accessed September 23, 2025, [https://www.pnas.org/doi/pdf/10.1073/pnas.2503187122](https://www.pnas.org/doi/pdf/10.1073/pnas.2503187122)  
44. \[PDF\] Guidelines for Human-AI Interaction | Semantic Scholar, accessed September 23, 2025, [https://www.semanticscholar.org/paper/Guidelines-for-Human-AI-Interaction-Amershi-Weld/ad3cf68bae32d21f25ac142287d4a556155619d2](https://www.semanticscholar.org/paper/Guidelines-for-Human-AI-Interaction-Amershi-Weld/ad3cf68bae32d21f25ac142287d4a556155619d2)  
45. Guidelines for Human-AI Interaction \- Microsoft, accessed September 23, 2025, [https://www.microsoft.com/en-us/research/wp-content/uploads/2019/01/Guidelines-for-Human-AI-Interaction-camera-ready.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2019/01/Guidelines-for-Human-AI-Interaction-camera-ready.pdf)  
46. Create human-centered AI with the Human-AI eXperience (HAX) Toolkit webinar \- Microsoft, accessed September 23, 2025, [https://www.microsoft.com/en-us/research/video/create-human-centered-ai-with-the-human-ai-experience-hax-toolkit-webinar/?locale=zh-cn](https://www.microsoft.com/en-us/research/video/create-human-centered-ai-with-the-human-ai-experience-hax-toolkit-webinar/?locale=zh-cn)  
47. Human-AI Interaction at ACM DIS 2024 | by Daniel Buschek \- Medium, accessed September 23, 2025, [https://medium.com/human-centered-ai/human-ai-interaction-at-acm-dis-2024-d39848536448](https://medium.com/human-centered-ai/human-ai-interaction-at-acm-dis-2024-d39848536448)  
48. What Is Human-Centered AI (HCAI)? — updated 2025 \- The Interaction Design Foundation, accessed September 23, 2025, [https://www.interaction-design.org/literature/topics/human-centered-ai](https://www.interaction-design.org/literature/topics/human-centered-ai)  
49. What is human-centered AI? \- IBM Research, accessed September 23, 2025, [https://research.ibm.com/blog/what-is-human-centered-ai](https://research.ibm.com/blog/what-is-human-centered-ai)  
50. Stimulating Cognitive Engagement in Hybrid ... \- CEUR-WS.org, accessed September 23, 2025, [https://ceur-ws.org/Vol-3825/prefaceW1.pdf](https://ceur-ws.org/Vol-3825/prefaceW1.pdf)  
51. Stimulating cognitive engagement in hybrid ... \- Milano-Bicocca, accessed September 23, 2025, [https://boa.unimib.it/retrieve/effe6d4b-32a8-48df-bada-79649c1dc105/Natali%20et%20al-2025-CEUR%20Workshop%20Proceedings%20Hybrid%20Human-Artificial%20Intelligence-PrePrint.pdf](https://boa.unimib.it/retrieve/effe6d4b-32a8-48df-bada-79649c1dc105/Natali%20et%20al-2025-CEUR%20Workshop%20Proceedings%20Hybrid%20Human-Artificial%20Intelligence-PrePrint.pdf)  
52. \[Call for Abstracts\] Second Workshop on Stimulating Cognitive Engagement in Hybrid Decision-Making: Friction, Reliance, and Biases (June 10 2025, Pisa, Italy \- co-located with HHAI25) \- GRIN Informatica, accessed September 23, 2025, [https://www.grin-informatica.it/call-for-abstracts-second-workshop-on-stimulating-cognitive-engagement-in-hybrid-decision-making-friction-reliance-and-biases-june-10-2025-pisa-italy-co-located-with-hhai25/](https://www.grin-informatica.it/call-for-abstracts-second-workshop-on-stimulating-cognitive-engagement-in-hybrid-decision-making-friction-reliance-and-biases-june-10-2025-pisa-italy-co-located-with-hhai25/)  
53. A Frictional Design Approach: Towards Judicial AI and its Possible Applications \- CEUR-WS, accessed September 23, 2025, [https://ceur-ws.org/Vol-3825/short1-2.pdf](https://ceur-ws.org/Vol-3825/short1-2.pdf)  
54. ‪Gagan Bansal‬ \- ‪Google Scholar‬, accessed September 23, 2025, [https://scholar.google.com/citations?user=eCSI4pMAAAAJ\&hl=en](https://scholar.google.com/citations?user=eCSI4pMAAAAJ&hl=en)  
55. The Evolution & Significance of AI Interfaces \- HCAI Institute, accessed September 23, 2025, [https://www.hcaiinstitute.com/blog/the\_evolution\_and\_significance\_of\_AI\_interfaces](https://www.hcaiinstitute.com/blog/the_evolution_and_significance_of_AI_interfaces)  
56. Human-Artificial Interaction in the Age of Generative AIs \- Frontiers, accessed September 23, 2025, [https://www.frontiersin.org/research-topics/66092/human-artificial-interaction-in-the-age-of-generative-ais](https://www.frontiersin.org/research-topics/66092/human-artificial-interaction-in-the-age-of-generative-ais)  
57. A Survey on Human-AI Collaboration with Large Foundation Models \- arXiv, accessed September 23, 2025, [https://arxiv.org/html/2403.04931v3](https://arxiv.org/html/2403.04931v3)  
58. The eleven commandments of AI UX. Sacred principles for the intelligence… \- UX Collective, accessed September 23, 2025, [https://uxdesign.cc/the-eleven-commandments-of-ai-ux-016bbea2dd9a](https://uxdesign.cc/the-eleven-commandments-of-ai-ux-016bbea2dd9a)  
59. Media Lab @ CHI 2021, accessed September 23, 2025, [https://www.media.mit.edu/events/media-lab-chi-2021/](https://www.media.mit.edu/events/media-lab-chi-2021/)  
60. Human-AI Interaction Workshop @ECAI2020 \- Google Sites, accessed September 23, 2025, [https://sites.google.com/view/human-ai-interaction-ecai2020/home](https://sites.google.com/view/human-ai-interaction-ecai2020/home)  
61. Stanford HAI: Home, accessed September 23, 2025, [https://hai.stanford.edu/](https://hai.stanford.edu/)  
62. Human-Computer Interaction Institute: Welcome to HCII, accessed September 23, 2025, [https://hcii.cmu.edu/](https://hcii.cmu.edu/)  
63. People \+ AI Research \- Google, accessed September 23, 2025, [https://pair.withgoogle.com/](https://pair.withgoogle.com/)  
64. Research Scientist Intern, Human Computer Interaction (PhD) \- Meta Careers, accessed September 23, 2025, [https://www.metacareers.com/jobs/1100790692182399](https://www.metacareers.com/jobs/1100790692182399)  
65. Human-AI eXperiences (HAX) team \- Microsoft Research, accessed September 23, 2025, [https://www.microsoft.com/en-us/research/group/hax-team/](https://www.microsoft.com/en-us/research/group/hax-team/)  
66. Human-Centered AI \- IBM Research, accessed September 23, 2025, [https://research.ibm.com/topics/human-centered-ai](https://research.ibm.com/topics/human-centered-ai)  
67. Survey X: Artificial Intelligence and the Future of Humans (Credited Responses), accessed September 23, 2025, [https://www.elon.edu/u/imagining/surveys/x-2018/credit/](https://www.elon.edu/u/imagining/surveys/x-2018/credit/)  
68. Artificial Intelligence and the Future of Humans \- NET, accessed September 23, 2025, [https://eloncdn.blob.core.windows.net/eu3/sites/964/2020/10/AI\_and\_the\_Future\_of\_Humans\_12\_10\_18.pdf](https://eloncdn.blob.core.windows.net/eu3/sites/964/2020/10/AI_and_the_Future_of_Humans_12_10_18.pdf)  
69. ‪Weizi Liu‬ \- ‪Google Scholar‬, accessed September 23, 2025, [https://scholar.google.com/citations?user=v-uPb-gAAAAJ\&hl=en](https://scholar.google.com/citations?user=v-uPb-gAAAAJ&hl=en)  
70. Against Softmaxing Culture: Understanding Relational Practices in Expert and Ordinary Forms of Work \- Microsoft Research, accessed September 23, 2025, [https://www.microsoft.com/en-us/research/video/against-softmaxing-culture-understanding-relational-practices-in-expert-and-ordinary-forms-of-work/?lang=fr\_ca\&locale=fr-ca](https://www.microsoft.com/en-us/research/video/against-softmaxing-culture-understanding-relational-practices-in-expert-and-ordinary-forms-of-work/?lang=fr_ca&locale=fr-ca)  
71. Against Softmaxing Culture: Understanding Relational Practices in Expert and Ordinary Forms of Work \- Microsoft Research, accessed September 23, 2025, [https://www.microsoft.com/en-us/research/video/against-softmaxing-culture-understanding-relational-practices-in-expert-and-ordinary-forms-of-work/?locale=ko-kr](https://www.microsoft.com/en-us/research/video/against-softmaxing-culture-understanding-relational-practices-in-expert-and-ordinary-forms-of-work/?locale=ko-kr)  
72. Agentic AI Ecosystems: Navigating Cultural-Awareness, Biases and Misinformation in Multi-agent and Human-agent Interactions \- Microsoft Research, accessed September 23, 2025, [https://www.microsoft.com/en-us/research/video/agentic-ai-ecosystems-navigating-cultural-awareness-biases-and-misinformation-in-multi-agent-and-human-agent-interactions/?locale=ko-kr](https://www.microsoft.com/en-us/research/video/agentic-ai-ecosystems-navigating-cultural-awareness-biases-and-misinformation-in-multi-agent-and-human-agent-interactions/?locale=ko-kr)  
73. Human-AI collaboration is not very collaborative yet: a ... \- Frontiers, accessed September 23, 2025, [https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2024.1521066/full](https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2024.1521066/full)  
74. Towards Interactive Evaluations for Interaction Harms in Human-AI Systems, accessed September 23, 2025, [https://knightcolumbia.org/content/towards-interactive-evaluations-for-interaction-harms-in-human-ai-systems](https://knightcolumbia.org/content/towards-interactive-evaluations-for-interaction-harms-in-human-ai-systems)  
75. Evaluating Human-AI Collaboration: A Review and Methodological Framework \- arXiv, accessed September 23, 2025, [https://arxiv.org/html/2407.19098v1](https://arxiv.org/html/2407.19098v1)