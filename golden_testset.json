{
    "user_input": "What role does Microsoft play in advancing autonomous AI agents?",
    "reference_contexts": [
        "## Emerging Frontiers and Future Research Trajectories As the capabilities of LLMs continue to advance at a rapid pace, the frontiers of human-AI interaction are constantly expanding. The research community is now looking beyond the immediate challenges of single-user, single-task interactions to consider more complex, long-term, and systemic questions about the future of AI-mediated work and society. This section synthesizes discussions from recent workshops, position papers, and forward-looking research to identify these emerging frontiers. ### The Evolution from Assistants to Autonomous Agents The dominant interaction paradigm is shifting from LLMs as assistive tools to agentic AI systems. These are systems capable of performing complex, multi-step tasks with a greater degree of autonomy, such as planning, tool use, and even collaborating with other AI agents.57 Frameworks like Microsoft's AutoGen are being developed to enable the creation of sophisticated multi-agent conversational systems that can tackle complex problems collaboratively.57 This evolution from assistant to agent has profound implications for human interaction. It raises new and urgent questions about human oversight, control handoffs, and shared accountability.60 The interaction model is transforming from one where a human directly instructs a tool to one where a human collaborates with, delegates to, and supervises a quasi-autonomous partner.22 Looking further ahead, interfaces may need to be designed with a dual audience in mind: they must be intuitive for humans while also being structured and machine-readable for other AI agents that may interact with them on a user's behalf.61 ### The Future of Human-Centered AI (HCAI) and Interaction Design The academic and industrial research communities are actively working to shape a human-centered future for AI. Major conferences in HCI and AI, such as CHI, CSCW, and AAAI, are now regular venues for workshops and panels dedicated to defining the future of HAI.47 A central theme of these discussions is the need to design AI that works *with* people to augment their capabilities, rather than simply working *for* them as a means of automation.62 This research is being led by a number of key institutional hubs, including university centers like Stanford's Institute for Human-Centered AI (HAI) and Carnegie Mellon's Human-Computer Interaction Institute (HCII), as well as dedicated teams within major corporate labs, such as Microsoft's Human-AI eXperiences (HAX) team, Google's People \\+ AI Research (PAIR) initiative, Meta's Reality Labs, and IBM's Human-Centered AI group.52 These groups champion interdisciplinary approaches and are pushing the boundaries of interaction design. The future of the AI interface is envisioned as moving beyond the simple chatbot paradigm toward more integrated and multimodal experiences: AI that is seamlessly embedded in existing workflows, hybrid interfaces that can fluidly switch between conversational and structured interaction, and multimodal systems that can understand and generate content across text, voice, visuals, and gestures.58 ### Long-Term Societal and Cognitive Impacts As generative AI becomes more deeply integrated into daily life, researchers are beginning to grapple with its potential long-term consequences for human cognition and society. A primary concern is the risk of skill atrophy and cognitive offloading. The \"irony of automation,\" first described by Bainbridge, posits that by automating routine tasks, a system deprives its human user of the regular practice needed to maintain their skills, leaving them unprepared to handle exceptions or situations where the automation fails.21 There is growing concern that over-dependence on LLMs for tasks like writing, problem-solving, and information synthesis will erode users' abilities to think critically and act independently.15 Another critical frontier is the need for more research into demographic and cultural considerations. Early studies are beginning to explore how factors like gender, age, and expertise level influence trust, reliance, and interaction patterns.22 For example, one study found significant gender differences in decision latency and cooperative attitudes toward different types of AI agents.22 Furthermore, as AI systems are deployed globally, designing culturally aware machines that can understand and respect local norms, values, and communication styles is a formidable but essential challenge for ensuring equitable and effective human-AI interaction.73 ###"
    ],
    "reference": "Microsoft is developing frameworks like AutoGen to enable the creation of sophisticated multi-agent conversational systems that can collaboratively tackle complex problems, marking a shift from LLMs as assistive tools to agentic AI systems capable of performing complex, multi-step tasks with greater autonomy.",
    "synthesizer_name": "single_hop_specific_query_synthesizer"
}
{
    "user_input": "What are the key research gaps in understanding human-LLM interaction that need to be addressed for better long-term and complex collaboration scenarios?",
    "reference_contexts": [
        "Identified Research Gaps and Future Directions This review of the literature reveals several critical gaps where future research is urgently needed: * Longitudinal Studies: The vast majority of current research is based on short-term, lab-based user studies. While valuable, these studies cannot capture how user behavior, skills, trust, and reliance patterns evolve over weeks, months, or years of sustained, real-world interaction with LLMs. There is a pressing need for long-term, longitudinal research to understand the dynamics of adoption and the potential for long-term cognitive effects. * Complex Collaboration Scenarios: The literature is heavily skewed toward studying single-user, single-AI interactions.76 The future of work, however, is likely to involve more complex scenarios, such as multiple human users collaborating with a single AI, or single users interacting with teams of specialized AI agents. Research must begin to explore these more complex topologies of human-AI teaming. * Standardized Methodologies and Metrics: The field currently lacks standardized methods and metrics for evaluating human-AI collaboration. This makes it difficult to compare findings across studies and to build a cumulative body of knowledge. Future work should focus on developing and validating robust evaluation frameworks that capture not only task performance but also the quality of the interaction, the user's cognitive load, and other human-centered outcomes.60 * The Impact of Demographics and Expertise: While some studies have begun to explore the role of user characteristics, this area remains significantly under-investigated. Systematic research is required to understand how demographic factors (e.g., age, culture, cognitive style) and varying levels of both domain expertise and AI literacy shape the full spectrum of human-LLM interaction, from prompting strategies to reliance behaviors."
    ],
    "reference": "The key research gaps in understanding human-LLM interaction include the need for long-term, longitudinal studies to capture how user behavior, skills, trust, and reliance evolve over sustained real-world interaction; exploration of complex collaboration scenarios involving multiple human users with a single AI or users interacting with teams of specialized AI agents; development of standardized methodologies and metrics to evaluate human-AI collaboration comprehensively, including task performance, interaction quality, and cognitive load; and systematic research on the impact of demographics and expertise, such as age, culture, cognitive style, domain expertise, and AI literacy, on various aspects of human-LLM interaction.",
    "synthesizer_name": "single_hop_specific_query_synthesizer"
}
{
    "user_input": "From the perspective of an AI Usage Research Analyst, how does the Human-Computer Interaction Institute contribute to addressing the challenges of human-LLM interaction and advancing human-centered AI design?",
    "reference_contexts": [
        "## Synthesis and Concluding Remarks This comprehensive review of the literature on human-LLM interaction reveals a field grappling with the profound complexities of a new technological paradigm. The research paints a clear picture of the LLM as a double-edged sword: its unprecedented fluency and generative power offer transformative potential, yet these same qualities create a \"plausibility trap\" that exacerbates the risks of user overreliance and complicates the task of critical verification. The decision to trust and rely on an LLM is not a simple cognitive calculation but a deeply sociotechnical phenomenon, shaped as much by the user's social context and individual biases as by the AI's objective performance. Across diverse applications\u2014from writing code and academic papers to making high-stakes medical diagnoses\u2014a consistent behavioral shift is emerging. The user's primary role is evolving from that of a creator to a curator, where the most valuable human contribution is no longer the generation of initial content but the critical evaluation, refinement, and contextualization of AI-generated outputs. In response to the challenges this new dynamic presents, the field of interaction design is undergoing its own evolution. A new paradigm of \"frictional\" or \"dialectical\" design is emerging, one that deliberately challenges the long-held HCI goal of seamless efficiency. By intentionally introducing cognitive friction through interventions like second opinions and competing explanations, these new approaches aim to foster a more deliberative and reflective engagement, transforming the AI from a simple assistant into a partner that promotes better human thinking. Ultimately, the literature makes a compelling case that effective and responsible human-LLM interaction is not an inevitable outcome of developing more powerful models. Rather, it will be the product of deliberate, human-centered design. Achieving a future of productive and safe human-AI partnership requires a deep, empirically-grounded understanding of user psychology, a willingness to prioritize critical thinking over frictionless convenience, and an unwavering commitment to the HCAI principle of building systems that augment, rather than automate, human intelligence. To this end, the research community must move to address the critical gaps identified in this review\u2014particularly the need for more longitudinal, ecologically valid, and methodologically robust studies\u2014to guide the co-evolution of humans and AI toward a beneficial and collaborative future. #### eleven commandments of AI UX. Sacred principles for the intelligence\u2026 \\- UX Collective, accessed September 23, 2025, [https:\/\/uxdesign.cc\/the-eleven-commandments-of-ai-ux-016bbea2dd9a](https:\/\/uxdesign.cc\/the-eleven-commandments-of-ai-ux-016bbea2dd9a) 59. Media Lab @ CHI 2021, accessed September 23, 2025, [https:\/\/www.media.mit.edu\/events\/media-lab-chi-2021\/](https:\/\/www.media.mit.edu\/events\/media-lab-chi-2021\/) 60. Human-AI Interaction Workshop @ECAI2020 \\- Google Sites, accessed September 23, 2025, [https:\/\/sites.google.com\/view\/human-ai-interaction-ecai2020\/home](https:\/\/sites.google.com\/view\/human-ai-interaction-ecai2020\/home) 61. Stanford HAI: Home, accessed September 23, 2025, [https:\/\/hai.stanford.edu\/](https:\/\/hai.stanford.edu\/) 62. Human-Computer Interaction Institute: Welcome to HCII, accessed September 23, 2025, [https:\/\/hcii.cmu.edu\/](https:\/\/hcii.cmu.edu\/) 63. People \\+ AI Research \\- Google, accessed September 23, 2025, [https:\/\/pair.withgoogle.com\/](https:\/\/pair.withgoogle.com\/) 64. Research Scientist Intern, Human Computer Interaction (PhD) \\- Meta Careers, accessed September 23, 2025, [https:\/\/www.metacareers.com\/jobs\/1100790692182399](https:\/\/www.metacareers.com\/jobs\/1100790692182399) 65. Human-AI eXperiences (HAX) team \\- Microsoft Research, accessed September 23, 2025, [https:\/\/www.microsoft.com\/en-us\/research\/group\/hax-team\/](https:\/\/www.microsoft.com\/en-us\/research\/group\/hax-team\/) 66. Human-Centered AI \\- IBM Research, accessed September 23, 2025, [https:\/\/research.ibm.com\/topics\/human-centered-ai](https:\/\/research.ibm.com\/topics\/human-centered-ai) 67. Survey X: Artificial Intelligence and the Future of Humans (Credited Responses), accessed September 23, 2025, [https:\/\/www.elon.edu\/u\/imagining\/surveys\/x-2018\/credit\/](https:\/\/www.elon.edu\/u\/imagining\/surveys\/x-2018\/credit\/) 68. Artificial Intelligence and the Future of Humans \\- NET, accessed September 23, 2025, [https:\/\/eloncdn.blob.core.windows.net\/eu3\/sites\/964\/2020\/10\/AI\\_and\\_the\\_Future\\_of\\_Humans\\_12\\_10\\_18.pdf](https:\/\/eloncdn.blob.core.windows.net\/eu3\/sites\/964\/2020\/10\/AI_and_the_Future_of_Humans_12_10_18.pdf) 69. Weizi Liu \\- Google Scholar, accessed September 23, 2025, [https:\/\/scholar.google.com\/citations?user=v-uPb-gAAAAJ\\&hl=en](https:\/\/scholar.google.com\/citations?user=v-uPb-gAAAAJ&hl=en) 70. Against Softmaxing Culture: Understanding Relational Practices in Expert and Ordinary Forms of Work \\- Microsoft Research, accessed September 23, 2025, [https:\/\/www.microsoft.com\/en-us\/research\/video\/against-softmaxing-culture-understanding-relational-practices-in-expert-and-ordinary-forms-of-work\/?lang=fr\\_ca\\&locale=fr-ca](https:\/\/www.microsoft.com\/en-us\/research\/video\/against-softmaxing-culture-understanding-relational-practices-in-expert-and-ordinary-forms-of-work\/?lang=fr_ca&locale=fr-ca) 71. Against Softmaxing Culture: Understanding Relational Practices in Expert and Ordinary Forms of Work \\- Microsoft Research, accessed September 23, 2025, [https:\/\/www.microsoft.com\/en-us\/research\/video\/against-softmaxing-culture-understanding-relational-practices-in-expert-and-ordinary-forms-of-work\/?locale=ko-kr](https:\/\/www.microsoft.com\/en-us\/research\/video\/against-softmaxing-culture-understanding-relational-practices-in-expert-and-ordinary-forms-of-work\/?locale=ko-kr) 72. Agentic AI Ecosystems: Navigating Cultural-Awareness, Biases and Misinformation in Multi-agent and Human-agent Interactions \\- Microsoft Research, accessed September 23, 2025, [https:\/\/www.microsoft.com\/en-us\/research\/video\/agentic-ai-ecosystems-navigating-cultural-awareness-biases-and-misinformation-in-multi-agent-and-human-agent-interactions\/?locale=ko-kr](https:\/\/www.microsoft.com\/en-us\/research\/video\/agentic-ai-ecosystems-navigating-cultural-awareness-biases-and-misinformation-in-multi-agent-and-human-agent-interactions\/?locale=ko-kr) 73. Human-AI collaboration is not very collaborative yet: a ... \\- Frontiers, accessed September 23, 2025, [https:\/\/www.frontiersin.org\/journals\/computer-science\/articles\/10.3389\/fcomp.2024.1521066\/full](https:\/\/www.frontiersin.org\/journals\/computer-science\/articles\/10.3389\/fcomp.2024.1521066\/full) 74. Towards Interactive Evaluations for Interaction Harms in Human-AI Systems, accessed September 23, 2025, [https:\/\/knightcolumbia.org\/content\/towards-interactive-evaluations-for-interaction-harms-in-human-ai-systems](https:\/\/knightcolumbia.org\/content\/towards-interactive-evaluations-for-interaction-harms-in-human-ai-systems) 75. Evaluating Human-AI Collaboration: A Review and Methodological Framework \\- arXiv, accessed September 23, 2025, [https:\/\/arxiv.org\/html\/2407.19098v1](https:\/\/arxiv.org\/html\/2407.19098v1)"
    ],
    "reference": "The Human-Computer Interaction Institute (HCII) is referenced within a comprehensive review of human-LLM interaction literature that highlights the complexities of this emerging technological paradigm. The review emphasizes the need for deliberate, human-centered design to foster effective and responsible human-AI partnerships. HCII, as part of the broader research community, is implicated in efforts to evolve interaction design toward paradigms that introduce cognitive friction and promote critical evaluation rather than seamless efficiency. This aligns with the field's goal to augment human intelligence through systems that support critical thinking and reflective engagement. Moreover, the review calls for more longitudinal, ecologically valid, and methodologically robust studies to guide the co-evolution of humans and AI, a research agenda that HCII is well-positioned to support given its focus on human-computer interaction. Thus, HCII contributes by advancing empirical understanding and design principles that address the sociotechnical challenges of human-LLM interaction and promote human-centered AI.",
    "synthesizer_name": "single_hop_specific_query_synthesizer"
}
{
    "user_input": "What does the study \"Why Johnny Can't Prompt\" reveal about the challenges novice users face in prompt engineering?",
    "reference_contexts": [
        "## Deconstructing User Engagement: From Latent Intent to Observable Interaction While the previous section focused on the user's internal cognitive state of trust and reliance, this section examines their active role in shaping the interaction. It analyzes the process of translating a high-level goal into an effective prompt, the distinct behaviors of novice and expert users, and frameworks for modeling the granular patterns of interaction that constitute AI-assisted work. ### The Challenge of User Intent: Bridging the \"Gulf of Execution\" Effective human-LLM interaction begins with the successful communication of user intent. To systematically study this process, researchers have moved to develop formal classifications of the goals users bring to these systems. These taxonomies of user intent go beyond the traditional \"navigational, informational, transactional\" model used for web search, reflecting the broader capabilities of generative models.8 Common intent categories identified in the literature include information seeking, content generation, brainstorming, editing and refinement, summarization, instruction following, and open-ended conversation.8 Despite the development of these taxonomies, a primary source of user dissatisfaction is the difficulty of intent recognition, where LLMs frequently fail to accurately infer the user's goal from an ambiguous prompt.9 Empirical studies comparing different model versions show that while newer models like GPT-4 are generally better at recognizing common intents, they can paradoxically be outperformed by older models on less frequent or more nuanced intents.26 This challenge leads to a significant and counter-intuitive finding in the literature, which can be described as a \"paradox of explicitness.\" While the failure of novices to craft clear prompts suggests that making prompts more explicit should improve outcomes, studies on intent-based prompt reformulation show the opposite. In these studies, a user's ambiguous prompt is first classified to identify the latent intent, and then automatically rewritten into a more explicit, structured prompt. Surprisingly, users are often *less* satisfied with the LLM's response to the \"improved,\" reformulated prompt than they were with the response to their original, less-perfect query.9 This paradox suggests that the process of automatic reformulation may strip the prompt of subtle but important context that the user had implicitly encoded. It also points to the value users place on agency and the feeling of a collaborative dialogue; an overly mechanized prompt-response cycle may feel less satisfying even if it is technically more precise. The implication is that the most effective support tools may not be those that reformulate prompts *for* the user, but those that guide the user to reformulate prompts *themselves*, preserving their sense of control and authorship. ### Prompting as a Skill: The Novice-Expert Gap The ability to craft effective prompts\u2014often termed prompt engineering\u2014has emerged as a critical skill for leveraging LLMs. The seminal study \"Why Johnny Can't Prompt\" by Zamfirescu-Pereira and colleagues provides a rich, qualitative description of the specific ways in which non-AI experts struggle with this task.27 The core of the problem lies in mismatched mental models. Novice users intuitively approach prompting as if they are instructing another human. They use natural language with the expectation of human-like inference, common sense, and shared context. Consequently, they are often confused and frustrated when small, semantically trivial changes to a prompt lead to vastly different outputs, or when the LLM fails to grasp implicit instructions.28 This mismatch leads to a characteristic pattern of opportunistic versus systematic exploration. Instead of methodically testing a prompt strategy across different contexts to ensure its robustness, novices tend to engage in ad-hoc trial and error. They often over-generalize from a single successful or failed interaction, either abandoning a promising approach prematurely or declaring \"premature victory\" without verifying that the desired behavior is consistent.28 These findings underscore the urgent need for improved \"LLM literacy\" among the general public and for the development of better design tools.28 Such tools could help bridge the novice-expert gap by encouraging more systematic evaluation and helping users build more accurate mental models of the LLM's capabilities and limitations. A complementary approach is the development of co-audit tools, which shift the focus from perfecting the input prompt to helping the user more effectively scrutinize and verify the AI-generated output.31 ###"
    ],
    "reference": "The study \"Why Johnny Can't Prompt\" by Zamfirescu-Pereira and colleagues highlights that novice users struggle with prompt engineering primarily due to mismatched mental models. Novices tend to approach prompting as if instructing another human, expecting human-like inference, common sense, and shared context. This leads to confusion and frustration when minor semantic changes cause vastly different outputs or when the LLM fails to grasp implicit instructions. Novices typically engage in opportunistic, ad-hoc trial and error rather than systematic exploration, often over-generalizing from single interactions and either abandoning promising approaches prematurely or declaring premature victory without verifying consistent behavior. These challenges emphasize the need for improved LLM literacy and better design tools to bridge the novice-expert gap.",
    "synthesizer_name": "single_hop_specific_query_synthesizer"
}
{
    "user_input": "How do social influence and trust calibration interact to affect user reliance on AI systems, and what design strategies can mitigate automation bias by promoting critical thinking in this sociotechnical context?",
    "reference_contexts": [
        "<1-hop>\n\n## The Cognitive Landscape of Human-AI Collaboration: Trust, Reliance, and Decision-Making The efficacy of any human-AI partnership is fundamentally governed by a set of core psychological constructs that dictate how users perceive, evaluate, and ultimately act upon AI-generated outputs. This section deconstructs these foundational factors, exploring the concepts of trust and reliance, the quantitative models used to predict user behavior, and the profound influence of the surrounding context on decision-making. ### Defining the Core Constructs: Trust, Reliance, and Calibration A central theme in the HAI literature is the crucial distinction between maximizing user trust and fostering appropriate reliance. The ultimate goal of human-centered AI design is not to make users trust the system unconditionally, but to empower them to calibrate their reliance accurately\u2014accepting correct AI outputs while rejecting incorrect ones.11 This calibrated state is the cornerstone of effective human-AI team performance, as both excessive and insufficient trust can be equally detrimental to outcomes.13 The challenge of inappropriate reliance manifests in two primary forms. The first, overreliance, also known as automation bias, is the tendency for users to accept incorrect AI recommendations uncritically.11 This behavior is often driven by an overestimation of the AI's capabilities, a desire to minimize cognitive effort, or the sheer plausibility of the AI's output. The very nature of generative AI exacerbates this problem. Unlike traditional systems that might produce a clearly erroneous number, LLMs generate fluent, grammatically perfect, and highly convincing prose, which significantly increases the cognitive burden of verification and makes errors far more difficult to detect.15 This creates a \"plausibility trap,\" where the model's greatest strength\u2014its linguistic prowess\u2014becomes a primary vector for user error. Even skilled professionals are susceptible, especially under time pressure or without adequate training on the system's limitations.15 The second form of inappropriate reliance is under-reliance, or algorithmic aversion. This is the tendency for users to reject correct and beneficial AI advice, often after witnessing the system make a mistake.13 This behavior can lead users to perform worse than they would have with AI assistance, thereby negating the system's potential benefits. Both over- and under-reliance stem from a failure of calibrated trust, which is the ideal state where a user's confidence in the AI is dynamic and proportional to its demonstrated performance within a specific context. The literature consistently shows that users struggle to achieve this calibration because they often lack a clear mental model of what the AI can do, how well it performs its functions, and the underlying mechanisms of its operation.11 ### Modeling and Predicting Human Reliance in AI-Assisted Decision-Making To move beyond descriptive accounts and toward predictive understanding, researchers have developed quantitative models of human reliance. Early approaches often assumed that reliance is the result of a rational, analytical process, akin to a cost-benefit analysis.17 However, more recent work argues that this view is incomplete and that it is critical to capture the *affective process* that underlies the human-AI interaction. In a significant advancement, Li, Lu, and Yin proposed a Hidden Markov Model to formalize this process.17 This model characterizes the interaction as a sequence of events where a user's latent (unobservable) trust in the AI evolves based on their experiences, such as instances of agreement or disagreement with the AI's recommendations. The user's decision to rely on the AI at any given moment is then a probabilistic function of this hidden trust state. Evaluations on human-subject data demonstrate that this Markovian approach significantly outperforms various baseline models in accurately predicting reliance behavior, providing a powerful quantitative framework for understanding the fluid dynamics of trust.17 This modeling becomes particularly important in real-world scenarios where explicit performance feedback is limited or absent. Research by Lu and Yin shows that in such environments, users develop cognitive shortcuts or heuristics to gauge the AI's reliability.5 A key heuristic is using the level of human-AI agreement on tasks where the human is highly confident as a proxy for the AI's overall performance. While this can be an efficient strategy, it also creates a vulnerability; an adversary could strategically manipulate the AI's recommendations on these high-confidence tasks to artificially inflate or deflate a user's trust.5 Building on these findings, researchers have sought to develop unified frameworks that can \"decode AI's nudge\" by integrating these cognitive heuristics and contextual factors to generate more robust predictions of human behavior in AI-assisted decision-making.18 ###",
        "<2-hop>\n\nThe Influence of Context on Reliance and Trust The decision to rely on an AI does not occur in a vacuum. It is profoundly shaped by the sociotechnical context in which the interaction is embedded. An individual's cognitive state and direct experience with an AI are heavily mediated by external factors, revealing reliance to be a sociotechnical phenomenon, not merely a cognitive one. One of the most powerful contextual factors is social influence. Experimental work by Lu and colleagues demonstrates that users rely on AI-based credibility indicators significantly *more* when they are under the influence of their peers, even when the AI's advice is incorrect.5 This finding is critical, as it shows that social proof can override a user's direct assessment of an AI's performance. The dynamic becomes even more nuanced when social influence comes from multiple sources, such as layperson peers and credentialed experts. In these scenarios, the effect of an AI indicator is moderated by its level of agreement with the expert's judgment, suggesting that users engage in a complex process of weighing and integrating social and algorithmic signals.18 This implies that interventions aimed at fostering appropriate reliance cannot be confined to the user-AI interface alone; they must account for the broader information ecosystem and the social dynamics at play. Other contextual factors also play a significant role. The stakes of the decision and the type of task influence reliance patterns.11 A user might be more willing to trust an LLM for a low-stakes creative writing task than for a high-stakes medical diagnosis. Similarly, reliance behaviors differ between objective tasks with clear right-or-wrong answers and subjective tasks that involve matters of opinion or taste.11 Finally, group dynamics add another layer of complexity. Research comparing the performance of individuals versus groups in AI-assisted tasks, such as recidivism risk assessment, indicates that the collaborative process within a human team further mediates how AI advice is interpreted and used.18 | Category | Factor | Description | Effect on Reliance | Supporting Snippets | | :---- | :---- | :---- | :---- | :---- | | Human Factors | Domain Expertise | User's knowledge of the task domain. | Higher expertise can decrease overreliance but may also lead to under-reliance. | 12 | | | AI Literacy | User's understanding of how AI\/LLMs work. | Low AI literacy correlates with higher overreliance, especially when explanations are present. | 11 | | | Cognitive Biases | Inherent biases like automation bias and confirmation bias. | Increases overreliance by predisposing users to agree with the AI. | 13 | | | Self-Confidence | User's confidence in their own abilities. | Higher self-confidence is associated with more critical thinking and less overreliance. | 21 | | | Demographics | Factors like age and gender. | Some studies show gender differences in trust and decision latency. | 22 | | AI System Factors | Perceived Accuracy | The user's belief in the AI's correctness, based on past performance. | Higher perceived accuracy directly increases reliance. | 5 | | | Explanation Type | The style and content of explanations provided. | Persuasive explanations can increase overreliance; verification-focused ones can decrease it. | 12 | | | Uncertainty Expression | Whether the AI explicitly signals its uncertainty. | Explicit uncertainty reduces overreliance by prompting user verification. | 12 | | | Fluency & Plausibility | The linguistic quality of the generated output. | High fluency increases overreliance by making outputs seem more credible and harder to critique. | 12 | | | Declared Identity | Whether the AI is presented as human, a simple AI, or a complex LLM. | Declared identity significantly affects cooperation rates and trust. | 22 | | Task\/Context Factors | Task Difficulty\/Stakes | The complexity and consequences of the decision. | Higher stakes can increase scrutiny but also reliance if the user feels overwhelmed. | 11 | | | Social Influence | The presence of opinions from peers or experts. | Social influence generally increases reliance on AI recommendations. | 18 | | | Time Pressure | The amount of time available for the user to make a decision. | High time pressure increases overreliance as users take cognitive shortcuts. | 15 | | | Emotional Context | Whether the interaction is emotionally neutral or charged. | High-emotion contexts can trigger negative reactions to AI's human-like language. | 25 | | | | | | |",
        "<3-hop>\n\n## Designing for Effective Partnership: Principles and Mitigation Strategies The challenges and user behaviors identified in the preceding sections underscore the need for a deliberate and principled approach to the design of human-LLM interaction. The literature offers a rich set of guidelines, strategies, and emerging design paradigms aimed at mitigating the risks of inappropriate reliance and fostering a safer, more productive human-AI partnership. These approaches range from foundational, universally applicable principles to novel interventions designed to actively promote critical thinking. ### Foundational Design Principles for Human-AI Interaction A significant body of work has sought to synthesize decades of research into actionable guidelines for practitioners. The Guidelines for Human-AI Interaction, developed by researchers at Microsoft, represent one of the most comprehensive efforts in this area.47 This framework proposes 18 guidelines that cover the entire lifecycle of an interaction, grouped into four categories: Initially, During Interaction, When Wrong, and Over Time. Key principles include G1: *Make clear what the system can do*, which addresses the need to set accurate user expectations from the outset; G4: *Show contextually relevant information*, which helps the user understand the basis for the AI's output; and G12: *Support efficient correction*, which acknowledges that AI errors are inevitable and that users must have easy ways to override or fix them. These specific guidelines are part of a broader movement toward Human-Centered AI (HCAI). HCAI is a design philosophy that prioritizes human needs, values, and capabilities, with the goal of creating AI systems that augment and empower users rather than displacing or de-skilling them.50 This approach advocates for interdisciplinary collaboration, bringing together computer scientists with psychologists, ethicists, and domain experts to ensure that systems are transparent, fair, and inclusive.51 ### Strategies for Communicating Uncertainty and Calibrating Trust Given that overreliance is a primary risk, many design strategies focus on helping users better calibrate their trust by making the AI's limitations more salient. One common approach is to provide explanations for AI outputs. However, the literature reveals a complex \"transparency-reliance\" dilemma: more explanation does not always lead to better user decisions. While the intuitive assumption is that explaining an AI's reasoning will improve trust calibration, empirical studies show that some types of explanations can inadvertently *increase* overreliance.12 For instance, users with low AI literacy may misinterpret technical-sounding or numeric explanations as a sign of objective truth, while users with high literacy may be lulled into a false sense of security about their ability to debug the system.11 The effectiveness of an explanation hinges on its function. Explanations designed to persuade a user of the AI's correctness can be dangerous, whereas verification-focused explanations\u2014those that provide evidence, cite sources, or otherwise lower the user's cost of checking the work\u2014are more effective at reducing overreliance.12 The guiding design principle should therefore be not simply to \"explain more,\" but to \"explain in a way that empowers the user to challenge the AI.\" A more direct and often more effective strategy is to have the AI explicitly express its uncertainty. This can be achieved through various means, such as using first-person linguistic hedges (\"I'm not sure, but...\") 24, displaying numerical confidence scores alongside recommendations, or visually highlighting words or phrases in a generated text for which the model has low confidence.32 Multiple studies have demonstrated that these forms of uncertainty communication are highly effective, substantially increasing the rate at which users identify and correct incorrect information in AI outputs.32 ###",
        "<4-hop>\n\nInterventions to Promote Critical Thinking: Cognitive Forcing and Frictional Design Recognizing that users often default to fast, intuitive, and low-effort \"System 1\" thinking, a more assertive class of interventions aims to actively nudge them into a more slow, deliberative, and analytical \"System 2\" mindset. These interventions are often referred to as Cognitive Forcing Functions (CFFs).12 Examples include: * AI Self-Critiques: The system is prompted to generate not only an answer but also a critique of that answer, pointing out potential weaknesses, alternative perspectives, or missing information. This explicitly models critical thinking for the user.12 * Introducing Second Opinions: The interface provides the user with an independent second opinion, either from another AI model or from a repository of human expert decisions. This intervention has been shown to be highly effective at reducing overreliance by forcing the user to compare and reconcile conflicting advice.5 These interventions are part of a nascent but important design paradigm that can be termed \"Frictional AI\".53 This paradigm represents a radical departure from traditional HCI, which has long prioritized seamlessness, efficiency, and the reduction of cognitive load. Frictional AI argues that for generative AI, a certain amount of \"programmed inefficiency\" or \"cognitive friction\" is not only desirable but necessary for safety and effectiveness. By intentionally slowing down the interaction, these designs aim to prevent the uncritical acceptance of AI outputs and promote more thoughtful engagement. A prime example of a frictional design pattern is \"Judicial AI.\" Instead of providing a single, definitive answer, a Judicial AI system presents the user with well-reasoned arguments for multiple, competing hypotheses or outcomes.56 For instance, in a medical diagnosis task, it might provide the evidence supporting Disease A, and then separately provide the evidence supporting a different diagnosis, Disease B. This forces the user to move from the role of a passive recipient of an answer to an active judge who must weigh the evidence. This approach is explicitly designed to mitigate automation bias and preserve the user's sense of agency and responsibility.56 This shift in design philosophy\u2014from AI as an assistant to AI as a dialectical sparring partner\u2014may be one of the most important frontiers in creating truly collaborative and responsible human-AI systems. | Underlying Mechanism | Strategy | Description | Example Implementation | Supporting Snippets | | :---- | :---- | :---- | :---- | :---- | | Improving User's Mental Model | Transparency of Capabilities | Clearly communicate what the AI can and cannot do, including its limitations and error boundaries. | Onboarding tutorials; explicit statements like \"I am an AI and can make mistakes.\" | 12 | | | Uncertainty Highlighting | Visually or textually signal the AI's confidence in its output. | Highlighting low-confidence words in a summary; providing a confidence score with a prediction. | 12 | | | Verification-Focused Explanations | Provide explanations that help the user check the AI's work, rather than just persuade them it's correct. | Citing sources for factual claims; showing the data points that most influenced a classification. | 12 | | Forcing Deliberation (System 2 Thinking) | Cognitive Forcing Functions (CFFs) | Interventions designed to interrupt automatic acceptance and encourage critical thought. | Requiring a user to type a justification before accepting a high-stakes recommendation. | 12 | | | AI Self-Critique | Prompting the AI to generate potential flaws or alternative perspectives on its own output. | \"Here is my answer. A potential weakness is that I did not consider the economic impact.\" | 12 | | | Introducing Second Opinions | Presenting an independent opinion from another source to encourage comparison and reduce reliance on a single AI. | An \"Ask another AI\" button; showing a human expert's prior decision on a similar case. | 18 | | Structuring a Dialectical Process | Frictional Design | Intentionally adding \"programmed inefficiencies\" to slow down interaction and promote reflection. | Introducing a mandatory time delay before a critical AI-suggested action can be confirmed. | 53 | | | Devil's Advocate | The AI is programmed to actively challenge the user's or another AI's assumptions. | In a group meeting, an LLM-powered agent is assigned the role of finding flaws in the proposed plan. | 18 | | | Judicial AI | The AI presents well-reasoned arguments for multiple, competing hypotheses or outcomes. | For a medical diagnosis, the AI provides evidence for Disease A and counter-evidence, while also providing evidence for Disease B. | 56 | | | | | | |"
    ],
    "reference": "Social influence significantly impacts user reliance on AI systems by increasing trust in AI recommendations, even when those recommendations are incorrect. Experimental findings show that users rely more on AI credibility indicators under peer influence, and this effect is further moderated by agreement with expert opinions, indicating a complex integration of social and algorithmic signals. This dynamic complicates trust calibration, which is the ideal state where users' confidence in AI is proportional to its demonstrated performance. Failure to achieve calibrated trust leads to inappropriate reliance, including automation bias\u2014overreliance on incorrect AI outputs\u2014and under-reliance, or algorithmic aversion. To mitigate these risks, design strategies emphasize fostering critical thinking through interventions such as Cognitive Forcing Functions (CFFs). These include AI self-critiques that model critical evaluation, and introducing second opinions from independent AI models or human experts to encourage comparison and reduce blind acceptance. Additionally, frictional design patterns like Judicial AI intentionally slow down interactions by presenting multiple competing hypotheses, compelling users to actively weigh evidence rather than passively accept AI outputs. Foundational principles also recommend clear communication of AI capabilities and explicit expression of uncertainty to help users better understand limitations and calibrate trust. Together, these approaches address the sociotechnical nature of reliance by accounting for social influence and promoting a more reflective, deliberative user mindset that mitigates automation bias.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer"
}
{
    "user_input": "how HCAI principles from design guidelines help reduce overreliance on AI and what new design paradigm emerge to support human critical thinking in human-LLM interaction?",
    "reference_contexts": [
        "<1-hop>\n\n## Designing for Effective Partnership: Principles and Mitigation Strategies The challenges and user behaviors identified in the preceding sections underscore the need for a deliberate and principled approach to the design of human-LLM interaction. The literature offers a rich set of guidelines, strategies, and emerging design paradigms aimed at mitigating the risks of inappropriate reliance and fostering a safer, more productive human-AI partnership. These approaches range from foundational, universally applicable principles to novel interventions designed to actively promote critical thinking. ### Foundational Design Principles for Human-AI Interaction A significant body of work has sought to synthesize decades of research into actionable guidelines for practitioners. The Guidelines for Human-AI Interaction, developed by researchers at Microsoft, represent one of the most comprehensive efforts in this area.47 This framework proposes 18 guidelines that cover the entire lifecycle of an interaction, grouped into four categories: Initially, During Interaction, When Wrong, and Over Time. Key principles include G1: *Make clear what the system can do*, which addresses the need to set accurate user expectations from the outset; G4: *Show contextually relevant information*, which helps the user understand the basis for the AI's output; and G12: *Support efficient correction*, which acknowledges that AI errors are inevitable and that users must have easy ways to override or fix them. These specific guidelines are part of a broader movement toward Human-Centered AI (HCAI). HCAI is a design philosophy that prioritizes human needs, values, and capabilities, with the goal of creating AI systems that augment and empower users rather than displacing or de-skilling them.50 This approach advocates for interdisciplinary collaboration, bringing together computer scientists with psychologists, ethicists, and domain experts to ensure that systems are transparent, fair, and inclusive.51 ### Strategies for Communicating Uncertainty and Calibrating Trust Given that overreliance is a primary risk, many design strategies focus on helping users better calibrate their trust by making the AI's limitations more salient. One common approach is to provide explanations for AI outputs. However, the literature reveals a complex \"transparency-reliance\" dilemma: more explanation does not always lead to better user decisions. While the intuitive assumption is that explaining an AI's reasoning will improve trust calibration, empirical studies show that some types of explanations can inadvertently *increase* overreliance.12 For instance, users with low AI literacy may misinterpret technical-sounding or numeric explanations as a sign of objective truth, while users with high literacy may be lulled into a false sense of security about their ability to debug the system.11 The effectiveness of an explanation hinges on its function. Explanations designed to persuade a user of the AI's correctness can be dangerous, whereas verification-focused explanations\u2014those that provide evidence, cite sources, or otherwise lower the user's cost of checking the work\u2014are more effective at reducing overreliance.12 The guiding design principle should therefore be not simply to \"explain more,\" but to \"explain in a way that empowers the user to challenge the AI.\" A more direct and often more effective strategy is to have the AI explicitly express its uncertainty. This can be achieved through various means, such as using first-person linguistic hedges (\"I'm not sure, but...\") 24, displaying numerical confidence scores alongside recommendations, or visually highlighting words or phrases in a generated text for which the model has low confidence.32 Multiple studies have demonstrated that these forms of uncertainty communication are highly effective, substantially increasing the rate at which users identify and correct incorrect information in AI outputs.32 ###",
        "<2-hop>\n\n## Synthesis and Concluding Remarks This comprehensive review of the literature on human-LLM interaction reveals a field grappling with the profound complexities of a new technological paradigm. The research paints a clear picture of the LLM as a double-edged sword: its unprecedented fluency and generative power offer transformative potential, yet these same qualities create a \"plausibility trap\" that exacerbates the risks of user overreliance and complicates the task of critical verification. The decision to trust and rely on an LLM is not a simple cognitive calculation but a deeply sociotechnical phenomenon, shaped as much by the user's social context and individual biases as by the AI's objective performance. Across diverse applications\u2014from writing code and academic papers to making high-stakes medical diagnoses\u2014a consistent behavioral shift is emerging. The user's primary role is evolving from that of a creator to a curator, where the most valuable human contribution is no longer the generation of initial content but the critical evaluation, refinement, and contextualization of AI-generated outputs. In response to the challenges this new dynamic presents, the field of interaction design is undergoing its own evolution. A new paradigm of \"frictional\" or \"dialectical\" design is emerging, one that deliberately challenges the long-held HCI goal of seamless efficiency. By intentionally introducing cognitive friction through interventions like second opinions and competing explanations, these new approaches aim to foster a more deliberative and reflective engagement, transforming the AI from a simple assistant into a partner that promotes better human thinking. Ultimately, the literature makes a compelling case that effective and responsible human-LLM interaction is not an inevitable outcome of developing more powerful models. Rather, it will be the product of deliberate, human-centered design. Achieving a future of productive and safe human-AI partnership requires a deep, empirically-grounded understanding of user psychology, a willingness to prioritize critical thinking over frictionless convenience, and an unwavering commitment to the HCAI principle of building systems that augment, rather than automate, human intelligence. To this end, the research community must move to address the critical gaps identified in this review\u2014particularly the need for more longitudinal, ecologically valid, and methodologically robust studies\u2014to guide the co-evolution of humans and AI toward a beneficial and collaborative future. #### eleven commandments of AI UX. Sacred principles for the intelligence\u2026 \\- UX Collective, accessed September 23, 2025, [https:\/\/uxdesign.cc\/the-eleven-commandments-of-ai-ux-016bbea2dd9a](https:\/\/uxdesign.cc\/the-eleven-commandments-of-ai-ux-016bbea2dd9a) 59. Media Lab @ CHI 2021, accessed September 23, 2025, [https:\/\/www.media.mit.edu\/events\/media-lab-chi-2021\/](https:\/\/www.media.mit.edu\/events\/media-lab-chi-2021\/) 60. Human-AI Interaction Workshop @ECAI2020 \\- Google Sites, accessed September 23, 2025, [https:\/\/sites.google.com\/view\/human-ai-interaction-ecai2020\/home](https:\/\/sites.google.com\/view\/human-ai-interaction-ecai2020\/home) 61. Stanford HAI: Home, accessed September 23, 2025, [https:\/\/hai.stanford.edu\/](https:\/\/hai.stanford.edu\/) 62. Human-Computer Interaction Institute: Welcome to HCII, accessed September 23, 2025, [https:\/\/hcii.cmu.edu\/](https:\/\/hcii.cmu.edu\/) 63. People \\+ AI Research \\- Google, accessed September 23, 2025, [https:\/\/pair.withgoogle.com\/](https:\/\/pair.withgoogle.com\/) 64. Research Scientist Intern, Human Computer Interaction (PhD) \\- Meta Careers, accessed September 23, 2025, [https:\/\/www.metacareers.com\/jobs\/1100790692182399](https:\/\/www.metacareers.com\/jobs\/1100790692182399) 65. Human-AI eXperiences (HAX) team \\- Microsoft Research, accessed September 23, 2025, [https:\/\/www.microsoft.com\/en-us\/research\/group\/hax-team\/](https:\/\/www.microsoft.com\/en-us\/research\/group\/hax-team\/) 66. Human-Centered AI \\- IBM Research, accessed September 23, 2025, [https:\/\/research.ibm.com\/topics\/human-centered-ai](https:\/\/research.ibm.com\/topics\/human-centered-ai) 67. Survey X: Artificial Intelligence and the Future of Humans (Credited Responses), accessed September 23, 2025, [https:\/\/www.elon.edu\/u\/imagining\/surveys\/x-2018\/credit\/](https:\/\/www.elon.edu\/u\/imagining\/surveys\/x-2018\/credit\/) 68. Artificial Intelligence and the Future of Humans \\- NET, accessed September 23, 2025, [https:\/\/eloncdn.blob.core.windows.net\/eu3\/sites\/964\/2020\/10\/AI\\_and\\_the\\_Future\\_of\\_Humans\\_12\\_10\\_18.pdf](https:\/\/eloncdn.blob.core.windows.net\/eu3\/sites\/964\/2020\/10\/AI_and_the_Future_of_Humans_12_10_18.pdf) 69. Weizi Liu \\- Google Scholar, accessed September 23, 2025, [https:\/\/scholar.google.com\/citations?user=v-uPb-gAAAAJ\\&hl=en](https:\/\/scholar.google.com\/citations?user=v-uPb-gAAAAJ&hl=en) 70. Against Softmaxing Culture: Understanding Relational Practices in Expert and Ordinary Forms of Work \\- Microsoft Research, accessed September 23, 2025, [https:\/\/www.microsoft.com\/en-us\/research\/video\/against-softmaxing-culture-understanding-relational-practices-in-expert-and-ordinary-forms-of-work\/?lang=fr\\_ca\\&locale=fr-ca](https:\/\/www.microsoft.com\/en-us\/research\/video\/against-softmaxing-culture-understanding-relational-practices-in-expert-and-ordinary-forms-of-work\/?lang=fr_ca&locale=fr-ca) 71. Against Softmaxing Culture: Understanding Relational Practices in Expert and Ordinary Forms of Work \\- Microsoft Research, accessed September 23, 2025, [https:\/\/www.microsoft.com\/en-us\/research\/video\/against-softmaxing-culture-understanding-relational-practices-in-expert-and-ordinary-forms-of-work\/?locale=ko-kr](https:\/\/www.microsoft.com\/en-us\/research\/video\/against-softmaxing-culture-understanding-relational-practices-in-expert-and-ordinary-forms-of-work\/?locale=ko-kr) 72. Agentic AI Ecosystems: Navigating Cultural-Awareness, Biases and Misinformation in Multi-agent and Human-agent Interactions \\- Microsoft Research, accessed September 23, 2025, [https:\/\/www.microsoft.com\/en-us\/research\/video\/agentic-ai-ecosystems-navigating-cultural-awareness-biases-and-misinformation-in-multi-agent-and-human-agent-interactions\/?locale=ko-kr](https:\/\/www.microsoft.com\/en-us\/research\/video\/agentic-ai-ecosystems-navigating-cultural-awareness-biases-and-misinformation-in-multi-agent-and-human-agent-interactions\/?locale=ko-kr) 73. Human-AI collaboration is not very collaborative yet: a ... \\- Frontiers, accessed September 23, 2025, [https:\/\/www.frontiersin.org\/journals\/computer-science\/articles\/10.3389\/fcomp.2024.1521066\/full](https:\/\/www.frontiersin.org\/journals\/computer-science\/articles\/10.3389\/fcomp.2024.1521066\/full) 74. Towards Interactive Evaluations for Interaction Harms in Human-AI Systems, accessed September 23, 2025, [https:\/\/knightcolumbia.org\/content\/towards-interactive-evaluations-for-interaction-harms-in-human-ai-systems](https:\/\/knightcolumbia.org\/content\/towards-interactive-evaluations-for-interaction-harms-in-human-ai-systems) 75. Evaluating Human-AI Collaboration: A Review and Methodological Framework \\- arXiv, accessed September 23, 2025, [https:\/\/arxiv.org\/html\/2407.19098v1](https:\/\/arxiv.org\/html\/2407.19098v1)"
    ],
    "reference": "The HCAI principles, as outlined in foundational design guidelines such as Microsoft's 18 guidelines for human-AI interaction, emphasize making clear what the system can do, showing contextually relevant information, and supporting efficient correction. These principles help set accurate user expectations, provide transparency about AI outputs, and enable users to easily override or fix AI errors, thereby mitigating risks of inappropriate reliance. Additionally, strategies like explicitly communicating AI uncertainty through linguistic hedges, confidence scores, or highlighting low-confidence text segments empower users to better calibrate their trust and identify incorrect information. Beyond these principles, a new design paradigm called \"frictional\" or \"dialectical\" design is emerging. This approach intentionally introduces cognitive friction through interventions such as second opinions and competing explanations to foster more deliberative and reflective engagement. This paradigm challenges the traditional goal of seamless efficiency in human-computer interaction, aiming instead to transform AI from a simple assistant into a partner that promotes better human critical thinking and evaluation. Together, these HCAI-informed design strategies and emerging paradigms support safer, more productive human-LLM partnerships by prioritizing human needs, values, and capabilities.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer"
}
{
    "user_input": "How does the Human-Computer Interaction Institute contribute to shaping the future of human-AI interaction, particularly in the context of evolving AI systems from assistants to autonomous agents and the emerging design paradigms discussed in recent research?",
    "reference_contexts": [
        "<1-hop>\n\n## Emerging Frontiers and Future Research Trajectories As the capabilities of LLMs continue to advance at a rapid pace, the frontiers of human-AI interaction are constantly expanding. The research community is now looking beyond the immediate challenges of single-user, single-task interactions to consider more complex, long-term, and systemic questions about the future of AI-mediated work and society. This section synthesizes discussions from recent workshops, position papers, and forward-looking research to identify these emerging frontiers. ### The Evolution from Assistants to Autonomous Agents The dominant interaction paradigm is shifting from LLMs as assistive tools to agentic AI systems. These are systems capable of performing complex, multi-step tasks with a greater degree of autonomy, such as planning, tool use, and even collaborating with other AI agents.57 Frameworks like Microsoft's AutoGen are being developed to enable the creation of sophisticated multi-agent conversational systems that can tackle complex problems collaboratively.57 This evolution from assistant to agent has profound implications for human interaction. It raises new and urgent questions about human oversight, control handoffs, and shared accountability.60 The interaction model is transforming from one where a human directly instructs a tool to one where a human collaborates with, delegates to, and supervises a quasi-autonomous partner.22 Looking further ahead, interfaces may need to be designed with a dual audience in mind: they must be intuitive for humans while also being structured and machine-readable for other AI agents that may interact with them on a user's behalf.61 ### The Future of Human-Centered AI (HCAI) and Interaction Design The academic and industrial research communities are actively working to shape a human-centered future for AI. Major conferences in HCI and AI, such as CHI, CSCW, and AAAI, are now regular venues for workshops and panels dedicated to defining the future of HAI.47 A central theme of these discussions is the need to design AI that works *with* people to augment their capabilities, rather than simply working *for* them as a means of automation.62 This research is being led by a number of key institutional hubs, including university centers like Stanford's Institute for Human-Centered AI (HAI) and Carnegie Mellon's Human-Computer Interaction Institute (HCII), as well as dedicated teams within major corporate labs, such as Microsoft's Human-AI eXperiences (HAX) team, Google's People \\+ AI Research (PAIR) initiative, Meta's Reality Labs, and IBM's Human-Centered AI group.52 These groups champion interdisciplinary approaches and are pushing the boundaries of interaction design. The future of the AI interface is envisioned as moving beyond the simple chatbot paradigm toward more integrated and multimodal experiences: AI that is seamlessly embedded in existing workflows, hybrid interfaces that can fluidly switch between conversational and structured interaction, and multimodal systems that can understand and generate content across text, voice, visuals, and gestures.58 ### Long-Term Societal and Cognitive Impacts As generative AI becomes more deeply integrated into daily life, researchers are beginning to grapple with its potential long-term consequences for human cognition and society. A primary concern is the risk of skill atrophy and cognitive offloading. The \"irony of automation,\" first described by Bainbridge, posits that by automating routine tasks, a system deprives its human user of the regular practice needed to maintain their skills, leaving them unprepared to handle exceptions or situations where the automation fails.21 There is growing concern that over-dependence on LLMs for tasks like writing, problem-solving, and information synthesis will erode users' abilities to think critically and act independently.15 Another critical frontier is the need for more research into demographic and cultural considerations. Early studies are beginning to explore how factors like gender, age, and expertise level influence trust, reliance, and interaction patterns.22 For example, one study found significant gender differences in decision latency and cooperative attitudes toward different types of AI agents.22 Furthermore, as AI systems are deployed globally, designing culturally aware machines that can understand and respect local norms, values, and communication styles is a formidable but essential challenge for ensuring equitable and effective human-AI interaction.73 ###",
        "<2-hop>\n\n## Synthesis and Concluding Remarks This comprehensive review of the literature on human-LLM interaction reveals a field grappling with the profound complexities of a new technological paradigm. The research paints a clear picture of the LLM as a double-edged sword: its unprecedented fluency and generative power offer transformative potential, yet these same qualities create a \"plausibility trap\" that exacerbates the risks of user overreliance and complicates the task of critical verification. The decision to trust and rely on an LLM is not a simple cognitive calculation but a deeply sociotechnical phenomenon, shaped as much by the user's social context and individual biases as by the AI's objective performance. Across diverse applications\u2014from writing code and academic papers to making high-stakes medical diagnoses\u2014a consistent behavioral shift is emerging. The user's primary role is evolving from that of a creator to a curator, where the most valuable human contribution is no longer the generation of initial content but the critical evaluation, refinement, and contextualization of AI-generated outputs. In response to the challenges this new dynamic presents, the field of interaction design is undergoing its own evolution. A new paradigm of \"frictional\" or \"dialectical\" design is emerging, one that deliberately challenges the long-held HCI goal of seamless efficiency. By intentionally introducing cognitive friction through interventions like second opinions and competing explanations, these new approaches aim to foster a more deliberative and reflective engagement, transforming the AI from a simple assistant into a partner that promotes better human thinking. Ultimately, the literature makes a compelling case that effective and responsible human-LLM interaction is not an inevitable outcome of developing more powerful models. Rather, it will be the product of deliberate, human-centered design. Achieving a future of productive and safe human-AI partnership requires a deep, empirically-grounded understanding of user psychology, a willingness to prioritize critical thinking over frictionless convenience, and an unwavering commitment to the HCAI principle of building systems that augment, rather than automate, human intelligence. To this end, the research community must move to address the critical gaps identified in this review\u2014particularly the need for more longitudinal, ecologically valid, and methodologically robust studies\u2014to guide the co-evolution of humans and AI toward a beneficial and collaborative future. #### eleven commandments of AI UX. Sacred principles for the intelligence\u2026 \\- UX Collective, accessed September 23, 2025, [https:\/\/uxdesign.cc\/the-eleven-commandments-of-ai-ux-016bbea2dd9a](https:\/\/uxdesign.cc\/the-eleven-commandments-of-ai-ux-016bbea2dd9a) 59. Media Lab @ CHI 2021, accessed September 23, 2025, [https:\/\/www.media.mit.edu\/events\/media-lab-chi-2021\/](https:\/\/www.media.mit.edu\/events\/media-lab-chi-2021\/) 60. Human-AI Interaction Workshop @ECAI2020 \\- Google Sites, accessed September 23, 2025, [https:\/\/sites.google.com\/view\/human-ai-interaction-ecai2020\/home](https:\/\/sites.google.com\/view\/human-ai-interaction-ecai2020\/home) 61. Stanford HAI: Home, accessed September 23, 2025, [https:\/\/hai.stanford.edu\/](https:\/\/hai.stanford.edu\/) 62. Human-Computer Interaction Institute: Welcome to HCII, accessed September 23, 2025, [https:\/\/hcii.cmu.edu\/](https:\/\/hcii.cmu.edu\/) 63. People \\+ AI Research \\- Google, accessed September 23, 2025, [https:\/\/pair.withgoogle.com\/](https:\/\/pair.withgoogle.com\/) 64. Research Scientist Intern, Human Computer Interaction (PhD) \\- Meta Careers, accessed September 23, 2025, [https:\/\/www.metacareers.com\/jobs\/1100790692182399](https:\/\/www.metacareers.com\/jobs\/1100790692182399) 65. Human-AI eXperiences (HAX) team \\- Microsoft Research, accessed September 23, 2025, [https:\/\/www.microsoft.com\/en-us\/research\/group\/hax-team\/](https:\/\/www.microsoft.com\/en-us\/research\/group\/hax-team\/) 66. Human-Centered AI \\- IBM Research, accessed September 23, 2025, [https:\/\/research.ibm.com\/topics\/human-centered-ai](https:\/\/research.ibm.com\/topics\/human-centered-ai) 67. Survey X: Artificial Intelligence and the Future of Humans (Credited Responses), accessed September 23, 2025, [https:\/\/www.elon.edu\/u\/imagining\/surveys\/x-2018\/credit\/](https:\/\/www.elon.edu\/u\/imagining\/surveys\/x-2018\/credit\/) 68. Artificial Intelligence and the Future of Humans \\- NET, accessed September 23, 2025, [https:\/\/eloncdn.blob.core.windows.net\/eu3\/sites\/964\/2020\/10\/AI\\_and\\_the\\_Future\\_of\\_Humans\\_12\\_10\\_18.pdf](https:\/\/eloncdn.blob.core.windows.net\/eu3\/sites\/964\/2020\/10\/AI_and_the_Future_of_Humans_12_10_18.pdf) 69. Weizi Liu \\- Google Scholar, accessed September 23, 2025, [https:\/\/scholar.google.com\/citations?user=v-uPb-gAAAAJ\\&hl=en](https:\/\/scholar.google.com\/citations?user=v-uPb-gAAAAJ&hl=en) 70. Against Softmaxing Culture: Understanding Relational Practices in Expert and Ordinary Forms of Work \\- Microsoft Research, accessed September 23, 2025, [https:\/\/www.microsoft.com\/en-us\/research\/video\/against-softmaxing-culture-understanding-relational-practices-in-expert-and-ordinary-forms-of-work\/?lang=fr\\_ca\\&locale=fr-ca](https:\/\/www.microsoft.com\/en-us\/research\/video\/against-softmaxing-culture-understanding-relational-practices-in-expert-and-ordinary-forms-of-work\/?lang=fr_ca&locale=fr-ca) 71. Against Softmaxing Culture: Understanding Relational Practices in Expert and Ordinary Forms of Work \\- Microsoft Research, accessed September 23, 2025, [https:\/\/www.microsoft.com\/en-us\/research\/video\/against-softmaxing-culture-understanding-relational-practices-in-expert-and-ordinary-forms-of-work\/?locale=ko-kr](https:\/\/www.microsoft.com\/en-us\/research\/video\/against-softmaxing-culture-understanding-relational-practices-in-expert-and-ordinary-forms-of-work\/?locale=ko-kr) 72. Agentic AI Ecosystems: Navigating Cultural-Awareness, Biases and Misinformation in Multi-agent and Human-agent Interactions \\- Microsoft Research, accessed September 23, 2025, [https:\/\/www.microsoft.com\/en-us\/research\/video\/agentic-ai-ecosystems-navigating-cultural-awareness-biases-and-misinformation-in-multi-agent-and-human-agent-interactions\/?locale=ko-kr](https:\/\/www.microsoft.com\/en-us\/research\/video\/agentic-ai-ecosystems-navigating-cultural-awareness-biases-and-misinformation-in-multi-agent-and-human-agent-interactions\/?locale=ko-kr) 73. Human-AI collaboration is not very collaborative yet: a ... \\- Frontiers, accessed September 23, 2025, [https:\/\/www.frontiersin.org\/journals\/computer-science\/articles\/10.3389\/fcomp.2024.1521066\/full](https:\/\/www.frontiersin.org\/journals\/computer-science\/articles\/10.3389\/fcomp.2024.1521066\/full) 74. Towards Interactive Evaluations for Interaction Harms in Human-AI Systems, accessed September 23, 2025, [https:\/\/knightcolumbia.org\/content\/towards-interactive-evaluations-for-interaction-harms-in-human-ai-systems](https:\/\/knightcolumbia.org\/content\/towards-interactive-evaluations-for-interaction-harms-in-human-ai-systems) 75. Evaluating Human-AI Collaboration: A Review and Methodological Framework \\- arXiv, accessed September 23, 2025, [https:\/\/arxiv.org\/html\/2407.19098v1](https:\/\/arxiv.org\/html\/2407.19098v1)"
    ],
    "reference": "The Human-Computer Interaction Institute (HCII) at Carnegie Mellon University is one of the key institutional hubs leading research to shape a human-centered future for AI. HCII actively participates in interdisciplinary efforts to push the boundaries of interaction design, focusing on AI systems that work with people to augment their capabilities rather than merely automate tasks. This aligns with the broader evolution from AI as simple assistants to more autonomous, agentic systems capable of complex, multi-step tasks and collaboration. HCII's work supports the development of interfaces that are intuitive for humans and structured for AI agents, addressing challenges such as human oversight, control handoffs, and shared accountability. Furthermore, HCII contributes to the emerging design paradigm of \"frictional\" or \"dialectical\" interaction, which intentionally introduces cognitive friction to promote critical evaluation and reflective engagement with AI outputs. This approach aims to transform AI from a mere assistant into a partner that fosters better human thinking, emphasizing the importance of deliberate, human-centered design to ensure effective and responsible human-AI partnerships.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer"
}
{
    "user_input": "What foundational design principles and identified research gaps are crucial for improving human-LLM interaction?",
    "reference_contexts": [
        "<1-hop>\n\n## Designing for Effective Partnership: Principles and Mitigation Strategies The challenges and user behaviors identified in the preceding sections underscore the need for a deliberate and principled approach to the design of human-LLM interaction. The literature offers a rich set of guidelines, strategies, and emerging design paradigms aimed at mitigating the risks of inappropriate reliance and fostering a safer, more productive human-AI partnership. These approaches range from foundational, universally applicable principles to novel interventions designed to actively promote critical thinking. ### Foundational Design Principles for Human-AI Interaction A significant body of work has sought to synthesize decades of research into actionable guidelines for practitioners. The Guidelines for Human-AI Interaction, developed by researchers at Microsoft, represent one of the most comprehensive efforts in this area.47 This framework proposes 18 guidelines that cover the entire lifecycle of an interaction, grouped into four categories: Initially, During Interaction, When Wrong, and Over Time. Key principles include G1: *Make clear what the system can do*, which addresses the need to set accurate user expectations from the outset; G4: *Show contextually relevant information*, which helps the user understand the basis for the AI's output; and G12: *Support efficient correction*, which acknowledges that AI errors are inevitable and that users must have easy ways to override or fix them. These specific guidelines are part of a broader movement toward Human-Centered AI (HCAI). HCAI is a design philosophy that prioritizes human needs, values, and capabilities, with the goal of creating AI systems that augment and empower users rather than displacing or de-skilling them.50 This approach advocates for interdisciplinary collaboration, bringing together computer scientists with psychologists, ethicists, and domain experts to ensure that systems are transparent, fair, and inclusive.51 ### Strategies for Communicating Uncertainty and Calibrating Trust Given that overreliance is a primary risk, many design strategies focus on helping users better calibrate their trust by making the AI's limitations more salient. One common approach is to provide explanations for AI outputs. However, the literature reveals a complex \"transparency-reliance\" dilemma: more explanation does not always lead to better user decisions. While the intuitive assumption is that explaining an AI's reasoning will improve trust calibration, empirical studies show that some types of explanations can inadvertently *increase* overreliance.12 For instance, users with low AI literacy may misinterpret technical-sounding or numeric explanations as a sign of objective truth, while users with high literacy may be lulled into a false sense of security about their ability to debug the system.11 The effectiveness of an explanation hinges on its function. Explanations designed to persuade a user of the AI's correctness can be dangerous, whereas verification-focused explanations\u2014those that provide evidence, cite sources, or otherwise lower the user's cost of checking the work\u2014are more effective at reducing overreliance.12 The guiding design principle should therefore be not simply to \"explain more,\" but to \"explain in a way that empowers the user to challenge the AI.\" A more direct and often more effective strategy is to have the AI explicitly express its uncertainty. This can be achieved through various means, such as using first-person linguistic hedges (\"I'm not sure, but...\") 24, displaying numerical confidence scores alongside recommendations, or visually highlighting words or phrases in a generated text for which the model has low confidence.32 Multiple studies have demonstrated that these forms of uncertainty communication are highly effective, substantially increasing the rate at which users identify and correct incorrect information in AI outputs.32 ###",
        "<2-hop>\n\nIdentified Research Gaps and Future Directions This review of the literature reveals several critical gaps where future research is urgently needed: * Longitudinal Studies: The vast majority of current research is based on short-term, lab-based user studies. While valuable, these studies cannot capture how user behavior, skills, trust, and reliance patterns evolve over weeks, months, or years of sustained, real-world interaction with LLMs. There is a pressing need for long-term, longitudinal research to understand the dynamics of adoption and the potential for long-term cognitive effects. * Complex Collaboration Scenarios: The literature is heavily skewed toward studying single-user, single-AI interactions.76 The future of work, however, is likely to involve more complex scenarios, such as multiple human users collaborating with a single AI, or single users interacting with teams of specialized AI agents. Research must begin to explore these more complex topologies of human-AI teaming. * Standardized Methodologies and Metrics: The field currently lacks standardized methods and metrics for evaluating human-AI collaboration. This makes it difficult to compare findings across studies and to build a cumulative body of knowledge. Future work should focus on developing and validating robust evaluation frameworks that capture not only task performance but also the quality of the interaction, the user's cognitive load, and other human-centered outcomes.60 * The Impact of Demographics and Expertise: While some studies have begun to explore the role of user characteristics, this area remains significantly under-investigated. Systematic research is required to understand how demographic factors (e.g., age, culture, cognitive style) and varying levels of both domain expertise and AI literacy shape the full spectrum of human-LLM interaction, from prompting strategies to reliance behaviors."
    ],
    "reference": "Foundational design principles for human-LLM interaction emphasize a deliberate and principled approach to mitigate risks such as inappropriate reliance and to foster a safer, more productive partnership. Key guidelines include making clear what the system can do to set accurate user expectations (G1), showing contextually relevant information to help users understand AI outputs (G4), and supporting efficient correction to allow users to easily override AI errors (G12). These principles are part of the broader Human-Centered AI philosophy, which prioritizes human needs, values, and capabilities, advocating interdisciplinary collaboration to ensure transparency, fairness, and inclusivity. Additionally, strategies for communicating uncertainty, such as expressing AI uncertainty explicitly and providing verification-focused explanations, help users better calibrate trust and reduce overreliance. Regarding research gaps, there is a pressing need for longitudinal studies to understand long-term user behavior and cognitive effects in real-world interactions with LLMs. Research must also explore complex collaboration scenarios involving multiple users or AI agents, develop standardized methodologies and metrics for evaluating human-AI collaboration beyond task performance, and investigate the impact of demographics and expertise on human-LLM interaction, including how factors like age, culture, cognitive style, domain expertise, and AI literacy influence user behaviors and reliance.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer"
}
{
    "user_input": "How do the identified research gaps in longitudinal studies and complex collaboration scenarios relate to the challenges of achieving calibrated trust and reliance in human-AI collaboration?",
    "reference_contexts": [
        "<1-hop>\n\nIdentified Research Gaps and Future Directions This review of the literature reveals several critical gaps where future research is urgently needed: * Longitudinal Studies: The vast majority of current research is based on short-term, lab-based user studies. While valuable, these studies cannot capture how user behavior, skills, trust, and reliance patterns evolve over weeks, months, or years of sustained, real-world interaction with LLMs. There is a pressing need for long-term, longitudinal research to understand the dynamics of adoption and the potential for long-term cognitive effects. * Complex Collaboration Scenarios: The literature is heavily skewed toward studying single-user, single-AI interactions.76 The future of work, however, is likely to involve more complex scenarios, such as multiple human users collaborating with a single AI, or single users interacting with teams of specialized AI agents. Research must begin to explore these more complex topologies of human-AI teaming. * Standardized Methodologies and Metrics: The field currently lacks standardized methods and metrics for evaluating human-AI collaboration. This makes it difficult to compare findings across studies and to build a cumulative body of knowledge. Future work should focus on developing and validating robust evaluation frameworks that capture not only task performance but also the quality of the interaction, the user's cognitive load, and other human-centered outcomes.60 * The Impact of Demographics and Expertise: While some studies have begun to explore the role of user characteristics, this area remains significantly under-investigated. Systematic research is required to understand how demographic factors (e.g., age, culture, cognitive style) and varying levels of both domain expertise and AI literacy shape the full spectrum of human-LLM interaction, from prompting strategies to reliance behaviors.",
        "<2-hop>\n\n## The Cognitive Landscape of Human-AI Collaboration: Trust, Reliance, and Decision-Making The efficacy of any human-AI partnership is fundamentally governed by a set of core psychological constructs that dictate how users perceive, evaluate, and ultimately act upon AI-generated outputs. This section deconstructs these foundational factors, exploring the concepts of trust and reliance, the quantitative models used to predict user behavior, and the profound influence of the surrounding context on decision-making. ### Defining the Core Constructs: Trust, Reliance, and Calibration A central theme in the HAI literature is the crucial distinction between maximizing user trust and fostering appropriate reliance. The ultimate goal of human-centered AI design is not to make users trust the system unconditionally, but to empower them to calibrate their reliance accurately\u2014accepting correct AI outputs while rejecting incorrect ones.11 This calibrated state is the cornerstone of effective human-AI team performance, as both excessive and insufficient trust can be equally detrimental to outcomes.13 The challenge of inappropriate reliance manifests in two primary forms. The first, overreliance, also known as automation bias, is the tendency for users to accept incorrect AI recommendations uncritically.11 This behavior is often driven by an overestimation of the AI's capabilities, a desire to minimize cognitive effort, or the sheer plausibility of the AI's output. The very nature of generative AI exacerbates this problem. Unlike traditional systems that might produce a clearly erroneous number, LLMs generate fluent, grammatically perfect, and highly convincing prose, which significantly increases the cognitive burden of verification and makes errors far more difficult to detect.15 This creates a \"plausibility trap,\" where the model's greatest strength\u2014its linguistic prowess\u2014becomes a primary vector for user error. Even skilled professionals are susceptible, especially under time pressure or without adequate training on the system's limitations.15 The second form of inappropriate reliance is under-reliance, or algorithmic aversion. This is the tendency for users to reject correct and beneficial AI advice, often after witnessing the system make a mistake.13 This behavior can lead users to perform worse than they would have with AI assistance, thereby negating the system's potential benefits. Both over- and under-reliance stem from a failure of calibrated trust, which is the ideal state where a user's confidence in the AI is dynamic and proportional to its demonstrated performance within a specific context. The literature consistently shows that users struggle to achieve this calibration because they often lack a clear mental model of what the AI can do, how well it performs its functions, and the underlying mechanisms of its operation.11 ### Modeling and Predicting Human Reliance in AI-Assisted Decision-Making To move beyond descriptive accounts and toward predictive understanding, researchers have developed quantitative models of human reliance. Early approaches often assumed that reliance is the result of a rational, analytical process, akin to a cost-benefit analysis.17 However, more recent work argues that this view is incomplete and that it is critical to capture the *affective process* that underlies the human-AI interaction. In a significant advancement, Li, Lu, and Yin proposed a Hidden Markov Model to formalize this process.17 This model characterizes the interaction as a sequence of events where a user's latent (unobservable) trust in the AI evolves based on their experiences, such as instances of agreement or disagreement with the AI's recommendations. The user's decision to rely on the AI at any given moment is then a probabilistic function of this hidden trust state. Evaluations on human-subject data demonstrate that this Markovian approach significantly outperforms various baseline models in accurately predicting reliance behavior, providing a powerful quantitative framework for understanding the fluid dynamics of trust.17 This modeling becomes particularly important in real-world scenarios where explicit performance feedback is limited or absent. Research by Lu and Yin shows that in such environments, users develop cognitive shortcuts or heuristics to gauge the AI's reliability.5 A key heuristic is using the level of human-AI agreement on tasks where the human is highly confident as a proxy for the AI's overall performance. While this can be an efficient strategy, it also creates a vulnerability; an adversary could strategically manipulate the AI's recommendations on these high-confidence tasks to artificially inflate or deflate a user's trust.5 Building on these findings, researchers have sought to develop unified frameworks that can \"decode AI's nudge\" by integrating these cognitive heuristics and contextual factors to generate more robust predictions of human behavior in AI-assisted decision-making.18 ###"
    ],
    "reference": "The identified research gaps highlight the need for long-term, longitudinal studies to understand how user behavior, skills, trust, and reliance patterns evolve over sustained, real-world interaction with large language models (LLMs). Current research is mostly short-term and lab-based, which limits insight into these dynamics. Additionally, the literature is heavily focused on single-user, single-AI interactions, whereas future work will involve more complex collaboration scenarios such as multiple humans working with a single AI or users interacting with teams of specialized AI agents. These gaps are directly related to the challenges of achieving calibrated trust and reliance in human-AI collaboration, as described in the cognitive landscape of trust, reliance, and decision-making. Calibrated trust requires users to dynamically adjust their confidence in AI outputs, accepting correct recommendations and rejecting incorrect ones. However, users often struggle with this calibration due to a lack of clear mental models of AI capabilities and the complexity introduced by generative AI's plausible outputs. Understanding how trust and reliance evolve over time and in complex collaborative settings is essential to address issues like overreliance (automation bias) and under-reliance (algorithmic aversion), which can both harm human-AI team performance. Therefore, addressing these research gaps is crucial for developing standardized methodologies and robust evaluation frameworks that capture not only task performance but also the quality of interaction and cognitive load in diverse human-AI collaboration scenarios.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer"
}