#!/usr/bin/env python3
"""
Golden Testset Management - Hybrid Prefect 3.x Flow
Combines clean execution core with optional enterprise features

This hybrid flow provides:
- Clean Prefect 3.x core (1:1 YAML mapping, explicit future resolution)
- Optional enterprise features: Git workflow, quality gates, monitoring, cost tracking
- Dual CLI modes: Simple development (`--only-phase`) and advanced production (`--production`)
- Flexible composition: Optional enterprise layers

Usage:
    # Simple development mode (clean core)
    python flows/golden_testset_flow.py --only-phase phase1

    # Enterprise mode with optional features
    python flows/golden_testset_flow.py --production --enable-quality-gates

    # Custom feature combination
    python flows/golden_testset_flow.py --enable-monitoring --enable-cost-tracking
"""

from __future__ import annotations

import os
import sys
import time
import yaml
import json
import signal
import typing as t
import subprocess
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass, field

from prefect import flow, task
from prefect.exceptions import PrefectException
from prefect.logging import get_run_logger
from prefect.tasks import task_input_hash
from prefect.artifacts import create_markdown_artifact

# ============================================================================
# Core Data Models (Clean Architecture)
# ============================================================================

@dataclass
class TaskSpec:
    id: str
    name: str
    run: t.List[str] = field(default_factory=list)
    verify: t.List[str] = field(default_factory=list)
    perf_budget_ms: t.Optional[int] = None
    accepts: t.List[str] = field(default_factory=list)
    artifacts: t.List[str] = field(default_factory=list)

@dataclass
class PhaseSpec:
    id: str
    name: str
    depends_on: t.List[str] = field(default_factory=list)
    tasks: t.List[TaskSpec] = field(default_factory=list)

@dataclass
class Plan:
    schema: str
    version: str
    project: str
    description: str
    providers: dict
    global_: dict
    phases: t.List[PhaseSpec]

@dataclass
class ExecutionConfig:
    """Configuration for optional enterprise features"""
    enable_git: bool = False
    enable_quality_gates: bool = False
    enable_monitoring: bool = False
    enable_cost_tracking: bool = False
    auto_merge: bool = False

# ============================================================================
# Core Utilities (Clean Implementation)
# ============================================================================

def load_yaml(path: Path) -> dict:
    """Load YAML configuration file"""
    with path.open("r", encoding="utf-8") as f:
        return yaml.safe_load(f)

def parse_plan(doc: dict) -> Plan:
    """Parse tasks.yaml into structured plan"""
    phases: t.List[PhaseSpec] = []
    for p in doc.get("phases", []):
        tasks = []
        for tsk in p.get("tasks", []):
            tasks.append(
                TaskSpec(
                    id=tsk["id"],
                    name=tsk.get("name", tsk["id"]),
                    run=list(tsk.get("run", []) or []),
                    verify=list(tsk.get("verify", []) or []),
                    perf_budget_ms=tsk.get("perf_budget_ms"),
                    accepts=list(tsk.get("accepts", []) or []),
                    artifacts=list(tsk.get("artifacts", []) or []),
                )
            )
        phases.append(
            PhaseSpec(
                id=p["id"],
                name=p.get("name", p["id"]),
                depends_on=list(p.get("depends_on", []) or []),
                tasks=tasks,
            )
        )
    return Plan(
        schema=doc.get("schema", ""),
        version=str(doc.get("version", "")),
        project=doc.get("project", ""),
        description=doc.get("description", ""),
        providers=doc.get("providers", {}),
        global_=doc.get("global", {}),
        phases=phases,
    )

def apply_globals(plan: Plan) -> None:
    """Apply global environment variables and validate secrets"""
    # Export environment variables (non-secret)
    for k, v in (plan.global_.get("env") or {}).items():
        os.environ[k] = os.path.expandvars(v) if isinstance(v, str) else str(v)

    # Ensure secrets exist in environment
    missing = [s for s in (plan.global_.get("secrets") or []) if os.environ.get(s) is None]
    if missing:
        raise RuntimeError(f"Missing required secrets in environment: {', '.join(missing)}")

def format_seconds(ms: float | None) -> str:
    """Format milliseconds as seconds string"""
    return f"{(ms or 0)/1000:.3f}s"

# ============================================================================
# Enterprise Features (Optional)
# ============================================================================

def create_git_branch(phase_id: str, config: ExecutionConfig) -> str:
    """Create feature branch for phase implementation"""
    if not config.enable_git:
        return "main"

    branch_name = f"feature/golden-testset-{phase_id}-{datetime.now().strftime('%Y%m%d-%H%M')}"
    subprocess.run(["git", "checkout", "-b", branch_name], check=True)
    return branch_name

def validate_quality_gates(plan: Plan, config: ExecutionConfig) -> bool:
    """Validate quality gates if enabled"""
    if not config.enable_quality_gates:
        return True

    logger = get_run_logger()
    gates = plan.global_.get("quality_gates", {})

    for category, checks in gates.items():
        logger.info(f"ðŸ” Validating {category} quality gates")
        for check in checks:
            logger.info(f"  âœ“ {check}")

    return True

def track_costs(operation: str, config: ExecutionConfig) -> None:
    """Track operation costs if enabled"""
    if not config.enable_cost_tracking:
        return

    logger = get_run_logger()
    cost_data = {
        "timestamp": datetime.now().isoformat(),
        "operation": operation,
        "estimated_cost": 0.02  # Placeholder
    }

    cost_file = Path("reports/cost_tracking.json")
    cost_file.parent.mkdir(exist_ok=True)

    costs = []
    if cost_file.exists():
        costs = json.loads(cost_file.read_text())

    costs.append(cost_data)
    cost_file.write_text(json.dumps(costs, indent=2))
    logger.info(f"ðŸ’° Cost tracked: {operation}")

def create_monitoring_report(outputs: list, config: ExecutionConfig) -> None:
    """Create monitoring report if enabled"""
    if not config.enable_monitoring:
        return

    logger = get_run_logger()
    report = {
        "timestamp": datetime.now().isoformat(),
        "total_phases": len(outputs),
        "total_tasks": sum(len(phase.get("tasks", [])) for phase in outputs),
        "status": "success"
    }

    reports_dir = Path("reports")
    reports_dir.mkdir(exist_ok=True)

    report_file = reports_dir / f"execution_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    report_file.write_text(json.dumps(report, indent=2))

    # Create Prefect artifact
    create_markdown_artifact(
        key="execution-summary",
        markdown=f"""# Execution Summary

**Timestamp**: {report['timestamp']}
**Phases Completed**: {report['total_phases']}
**Tasks Executed**: {report['total_tasks']}
**Status**: {report['status']}
        """,
        description="Golden testset execution summary"
    )

    logger.info(f"ðŸ“Š Monitoring report created: {report_file}")

# ============================================================================
# Core Execution Tasks (Clean Prefect 3.x)
# ============================================================================

@task(retries=1, retry_delay_seconds=5, cache_key_fn=task_input_hash)
def run_commands(cmds: t.List[str], cwd: str | None = None, extra_env: dict | None = None) -> dict:
    """
    Run a list of shell commands; fail-fast on first non-zero exit.
    Returns a dict with code/out/err/elapsed_ms.
    """
    logger = get_run_logger()
    start = time.perf_counter()
    last_out, last_err, code = "", "", 0

    for c in cmds:
        pretty = c if len(c) < 180 else c[:177] + "..."
        logger.info(f"$ {pretty}")
        proc = subprocess.Popen(
            c,
            cwd=cwd,
            env={**os.environ, **(extra_env or {})},
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            shell=True,
            preexec_fn=os.setsid,
        )
        out_b, err_b = proc.communicate()
        code = proc.returncode
        last_out, last_err = out_b.decode(errors="ignore"), err_b.decode(errors="ignore")
        if last_out.strip():
            logger.info(last_out.strip())
        if code != 0:
            if last_err.strip():
                logger.error(last_err.strip())
            try:
                os.killpg(proc.pid, signal.SIGTERM)
            except Exception:
                pass
            raise PrefectException(f"Command failed (exit {code}): {c}\n{last_err}")

    elapsed_ms = (time.perf_counter() - start) * 1000.0
    logger.info(f"Step completed in {format_seconds(elapsed_ms)}")
    return {"exit_code": code, "stdout": last_out, "stderr": last_err, "elapsed_ms": elapsed_ms}

# ============================================================================
# Flow Orchestration (Hybrid: Clean Core + Optional Enterprise)
# ============================================================================

@flow(name="golden_testset_task")
def run_task(task_spec: TaskSpec, workdir: str | None = None, config: ExecutionConfig = None) -> dict:
    """Execute a single task with optional enterprise features"""
    logger = get_run_logger()
    config = config or ExecutionConfig()

    logger.info(f"â–¶ Running task: {task_spec.id} â€” {task_spec.name}")

    # Enterprise: Cost tracking
    track_costs(f"task_{task_spec.id}", config)

    elapsed = 0.0

    # RUN commands â€” Prefect 3 requires explicit resolution of futures
    if task_spec.run:
        run_res = run_commands.submit(task_spec.run, cwd=workdir).result()
        elapsed = float(run_res.get("elapsed_ms", 0.0))
        if task_spec.perf_budget_ms and elapsed > task_spec.perf_budget_ms:
            raise PrefectException(
                f"Perf budget exceeded for {task_spec.id}: {format_seconds(elapsed)} > {format_seconds(task_spec.perf_budget_ms)}"
            )
    else:
        logger.warning("No 'run:' commands defined; skipping RUN step.")

    # VERIFY commands
    if task_spec.verify:
        run_commands.submit(task_spec.verify, cwd=workdir).result()
    else:
        logger.info("No 'verify:' commands defined; skipping VERIFY step.")

    # ACCEPTS (human-readable assertions)
    for cond in task_spec.accepts:
        logger.info(f"[accepts] {cond}")

    # ARTIFACTS (log presence)
    missing_artifacts = [p for p in task_spec.artifacts if not Path(p).exists()]
    if missing_artifacts:
        logger.warning(f"Declared artifacts missing: {missing_artifacts}")

    logger.info(f"âœ… Task complete: {task_spec.id}")
    return {"task_id": task_spec.id, "elapsed_ms": elapsed, "artifacts": task_spec.artifacts}

@flow(name="golden_testset_phase")
def run_phase(phase_spec: PhaseSpec, workdir: str | None = None, config: ExecutionConfig = None) -> dict:
    """Execute a phase with optional enterprise features"""
    logger = get_run_logger()
    config = config or ExecutionConfig()

    logger.info(f"=== PHASE {phase_spec.id}: {phase_spec.name} ===")

    # Enterprise: Git workflow
    branch_name = create_git_branch(phase_spec.id, config)
    if config.enable_git:
        logger.info(f"ðŸŒ¿ Working on branch: {branch_name}")

    results = []
    for tsk in phase_spec.tasks:
        res = run_task.submit(tsk, workdir=workdir, config=config).result()
        results.append(res)

    logger.info(f"=== PHASE {phase_spec.id} complete ===")
    return {"phase_id": phase_spec.id, "tasks": results, "branch": branch_name}

@flow(name="golden_testset_orchestrator")
def orchestrate(
    tasks_yaml_path: str = "tasks.yaml",
    workdir: str | None = ".",
    only_phase: str | None = None,
    only_task: str | None = None,
    config: ExecutionConfig = None,
) -> dict:
    """
    Hybrid orchestrator: Clean Prefect 3.x core with optional enterprise features
    """
    logger = get_run_logger()
    config = config or ExecutionConfig()

    # Load and parse plan (clean core)
    plan_doc = load_yaml(Path(tasks_yaml_path))
    plan = parse_plan(plan_doc)

    apply_globals(plan)
    logger.info(f"Project: {plan.project}  Version: {plan.version}")
    logger.info(f"Schema: {plan.schema}  Description: {plan.description}")

    # Enterprise: Quality gates validation
    validate_quality_gates(plan, config)

    # Validate phase ordering (clean core)
    index = {p.id: i for i, p in enumerate(plan.phases)}
    for p in plan.phases:
        for dep in p.depends_on:
            if index.get(dep, -1) >= index[p.id]:
                raise RuntimeError(f"Phase ordering invalid: {p.id} depends on {dep} which appears after it")

    outputs = []
    for p in plan.phases:
        if only_phase and p.id != only_phase:
            continue

        if only_task:
            target = next((t for t in p.tasks if t.id == only_task), None)
            if not target:
                raise RuntimeError(f"Task {only_task} not found in phase {p.id}")
            res = run_task.submit(target, workdir=workdir, config=config).result()
            outputs.append({"phase": p.id, "tasks": [res]})
            break
        else:
            res = run_phase.submit(p, workdir=workdir, config=config).result()
            outputs.append(res)

    # Enterprise: Monitoring and reporting
    create_monitoring_report(outputs, config)

    logger.info("ðŸŽ‰ Orchestration complete.")
    return {"outputs": outputs, "config": config.__dict__}

# ============================================================================
# CLI Interface (Dual Mode: Simple + Advanced)
# ============================================================================

if __name__ == "__main__":
    import argparse

    ap = argparse.ArgumentParser(description="Golden Testset Hybrid Orchestrator")

    # Core arguments (clean interface)
    ap.add_argument("--tasks", default=".claude/tasks.yaml", help="Path to tasks.yaml")
    ap.add_argument("--workdir", default=".", help="Working directory")
    ap.add_argument("--only-phase", default=None, help="Run only this phase")
    ap.add_argument("--only-task", default=None, help="Run only this task (requires --only-phase)")

    # Enterprise feature flags
    ap.add_argument("--enable-git", action="store_true", help="Enable Git workflow")
    ap.add_argument("--enable-quality-gates", action="store_true", help="Enable quality validation")
    ap.add_argument("--enable-monitoring", action="store_true", help="Enable reporting")
    ap.add_argument("--enable-cost-tracking", action="store_true", help="Enable cost tracking")
    ap.add_argument("--auto-merge", action="store_true", help="Auto-merge PRs (dev only)")

    # Preset modes
    ap.add_argument("--production", action="store_true", help="Enable all enterprise features")
    ap.add_argument("--development", action="store_true", help="Development mode with auto-merge")

    args = ap.parse_args()

    # Configure execution based on mode
    config = ExecutionConfig()

    if args.production:
        config.enable_git = True
        config.enable_quality_gates = True
        config.enable_monitoring = True
        config.enable_cost_tracking = True
        config.auto_merge = False  # Manual review in production
    elif args.development:
        config.enable_git = True
        config.enable_monitoring = True
        config.auto_merge = True
    else:
        # Individual feature flags
        config.enable_git = args.enable_git
        config.enable_quality_gates = args.enable_quality_gates
        config.enable_monitoring = args.enable_monitoring
        config.enable_cost_tracking = args.enable_cost_tracking
        config.auto_merge = args.auto_merge

    print(f"ðŸš€ Running with config: {config.__dict__}")

    # Execute orchestration
    orchestrate(
        tasks_yaml_path=args.tasks,
        workdir=args.workdir,
        only_phase=args.only_phase,
        only_task=args.only_task,
        config=config
    )